{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Intro","text":"<p>WGAN-GP with Meta-Feature Statistics (MFS)</p> <p>A PyTorch implementation of Wasserstein GAN with Gradient Penalty (WGAN-GP) enhanced with meta-feature statistics  preservation for synthetic tabular data generation.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>WGAN-GP Implementation: Stable GAN training with Wasserstein distance and gradient penalty</li> <li>Meta-Feature Statistics (MFS) Integration: Preserves statistical properties of the original data</li> <li>Comprehensive Evaluation: Multiple utility and fidelity metrics for synthetic data assessment</li> <li>Experiment Tracking: Built-in Aim tracking for monitoring training progress</li> <li>Flexible Architecture: Configurable generator and discriminator architectures</li> <li>Multi-Dataset Support: Tested on tabular datasets including Abalone and California Housing</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<p>The project implements two main training approaches:</p> <ol> <li>Vanilla WGAN-GP: Standard WGAN-GP implementation</li> <li>MFS-Enhanced WGAN-GP: WGAN-GP with additional meta-feature statistics preservation loss</li> </ol>"},{"location":"#key-components","title":"Key Components","text":"<ul> <li>Generator: Residual network-based generator with configurable dimensions</li> <li>Discriminator: Multi-layer discriminator with LeakyReLU activations</li> <li>MFS Manager: PyTorch implementation of meta-feature extraction (correlation, covariance, eigenvalues, etc.)</li> <li>Wasserstein Distance: Topological distance computation for MFS alignment</li> </ul>"},{"location":"#installation","title":"Installation","text":"<ol> <li> <p>Clone the repository: <pre><code>git clone https://github.com/Roman223/GAN_MFS\ncd GAN_MFS\n</code></pre></p> </li> <li> <p>Install dependencies: <pre><code>pip install -r req.txt\n</code></pre></p> </li> </ol>"},{"location":"#usage","title":"Usage","text":""},{"location":"#basic-training","title":"Basic Training","text":"<pre><code>from wgan_gp.models import Generator, Discriminator\nfrom wgan_gp.training import Trainer, TrainerModified\n\n# Configure hyperparameters\nlearning_params = {\n    'epochs': 1000,\n    'learning_rate_D': 0.0001,\n    'learning_rate_G': 0.0001,\n    'batch_size': 64,\n    'gp_weight': 10,\n    'generator_dim': (64, 128, 64),\n    'discriminator_dim': (64, 128, 64),\n    'emb_dim': 32,\n}\n\n# For MFS-enhanced training\nlearning_params.update({\n    'mfs_lambda': [0.1, 2.0],  # or single float value\n    'subset_mfs': ['mean', 'var', 'eigenvalues'],\n    'sample_number': 10,\n    'sample_frac': 0.5,\n})\n\n# Initialize and train model\ntrainer = TrainerModified(...)  # or Trainer() for vanilla WGAN-GP\ntrainer.train(data_loader, epochs, plot_freq=100)\n</code></pre>"},{"location":"#data-preparation","title":"Data Preparation","text":"<p>The project expects tabular data with the target variable. Example preprocessing:</p> <pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load your data\ndata = pd.read_csv('your_data.csv')\ny = data.pop('target').values\nX = data.values\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\n# Combine features and target for training\ntraining_data = np.hstack([X_train, y_train.reshape(-1, 1)])\n</code></pre>"},{"location":"#meta-feature-statistics-mfs","title":"Meta-Feature Statistics (MFS)","text":"<p>The MFS component preserves important statistical properties of the original data:</p>"},{"location":"#supported-meta-features","title":"Supported Meta-Features","text":"<ul> <li>Statistical: mean, variance, standard deviation, range, min, max, median</li> <li>Distributional: skewness, kurtosis, interquartile range</li> <li>Relational: correlation matrix, covariance matrix, eigenvalues</li> <li>Advanced: sparsity, mad (median absolute deviation)</li> </ul>"},{"location":"#mfs-loss-function","title":"MFS Loss Function","text":"<p>The generator loss combines adversarial loss with MFS preservation:</p> <pre><code>g_loss = d_generated.mean() + \u03bb * wasserstein_distance(mfs_real, mfs_synthetic)\n</code></pre>"},{"location":"#evaluation-metrics","title":"Evaluation Metrics","text":"<p>The project includes comprehensive evaluation metrics:</p>"},{"location":"#utility-metrics","title":"Utility Metrics","text":"<ul> <li>R\u00b2 Score: Regression performance preservation</li> <li>RMSE: Root mean squared error</li> <li>MAPE: Mean absolute percentage error</li> </ul>"},{"location":"#fidelity-metrics","title":"Fidelity Metrics","text":"<ul> <li>Correlation Matrix Distance: Cosine distance between correlation matrices</li> <li>Jensen-Shannon Divergence: Marginal distribution similarity</li> </ul>"},{"location":"#experiment-tracking","title":"Experiment Tracking","text":"<p>Aim tracking is implemented and strongly advised for monitoring:</p> <ul> <li>Training losses (Generator, Discriminator, MFS)</li> <li>Gradient norms and flow visualization</li> <li>Sample quality progression</li> <li>Comprehensive metrics logging</li> </ul> <p>To wake up server invoke: <pre><code>aim up\n</code></pre></p>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>wgan_gp/\n    __init__.py\n    main.py                 # Main training script\n    models.py               # Generator and Discriminator definitions\n    training.py             # Training loops and MFS integration\n    pymfe_to_torch.py       # PyTorch meta-feature extraction\n    utils.py                # Utility functions and metrics\n    req.txt                 # Dependencies\n    gitignore               # Git ignore file\n    README.md               # This file\n</code></pre>"},{"location":"#key-dependencies","title":"Key Dependencies","text":"<ul> <li>PyTorch: Deep learning framework</li> <li>Aim: Experiment tracking</li> <li>scikit-learn: Machine learning utilities</li> <li>pandas/numpy: Data manipulation</li> <li>matplotlib/seaborn: Visualization</li> <li>torch-topological: Topological data analysis</li> <li>POT: Optimal transport (Wasserstein distance)</li> </ul>"},{"location":"#configuration","title":"Configuration","text":"<p>Key hyperparameters can be configured in <code>main.py</code>:</p> <pre><code>learning_params = dict(\n    epochs=1000,                    # Training epochs\n    learning_rate_D=0.0001,         # Discriminator learning rate\n    learning_rate_G=0.0001,         # Generator learning rate\n    batch_size=64,                  # Batch size\n    gp_weight=10,                   # Gradient penalty weight\n    generator_dim=(64,128,64),      # Generator architecture\n    discriminator_dim=(64,128,64),  # Discriminator architecture\n    mfs_lambda=0.1,           # MFS loss weights\n    subset_mfs=['mean','var'],      # Selected meta-features\n    sample_number=10,               # Number of variates for MFS\n    sample_frac=0.5,                # Fraction for variate sampling\n)\n</code></pre>"},{"location":"#results","title":"Results","text":"<p>The model generates high-quality synthetic tabular data that preserves:</p> <ul> <li>Statistical distributions of original features</li> <li>Correlational structure between variables</li> <li>Utility for downstream machine learning tasks</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please feel free to submit issues, feature requests, or pull requests.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use this code in your research, please cite:</p> <pre><code>@misc{wgan_gp_mfs,\n    title={WGAN-GP with Meta-Feature Statistics for Synthetic Tabular Data Generation},\n    author={[Your Name]},\n    year={2024},\n    url={https://github.com/your-username/your-repo}\n}\n</code></pre>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<ul> <li>Original WGAN-GP paper: Improved Training of Wasserstein GANs  </li> <li>PyMFE library for meta-feature extraction inspiration  </li> <li>Aim team for excellent experiment tracking tools</li> </ul>"},{"location":"utils/","title":"Utils","text":"<p>This module contains utility functions for data processing, metric calculation, and evaluation of synthetic data generation.</p>"},{"location":"utils/#wgan_gp.utils","title":"<code>wgan_gp.utils</code>","text":""},{"location":"utils/#wgan_gp.utils.calc_metrics","title":"<code>calc_metrics(synth, test)</code>","text":"<p>Calculates the cosine distance between the correlation matrices of a synthetic and a test dataset. This helps to quantify how well the synthetic data preserves the relationships between features present in the original data. Preserving these relationships is crucial for maintaining the utility of the synthetic data for downstream tasks.</p> <pre><code>Args:\n    synth: The synthetic dataset.\n    test: The test dataset (typically the real data).\n\nReturns:\n    dict: A dictionary containing the cosine distance between the correlation\n        matrices of the synthetic and test datasets, keyed by\n        \"cosine_dist_corr_matrix\". A lower distance indicates better preservation\n        of feature relationships.\n</code></pre> Source code in <code>wgan_gp/utils.py</code> <pre><code>def calc_metrics(synth, test):\n    \"\"\"\n    Calculates the cosine distance between the correlation matrices of a synthetic and a test dataset. This helps to quantify how well the synthetic data preserves the relationships between features present in the original data. Preserving these relationships is crucial for maintaining the utility of the synthetic data for downstream tasks.\n\n        Args:\n            synth: The synthetic dataset.\n            test: The test dataset (typically the real data).\n\n        Returns:\n            dict: A dictionary containing the cosine distance between the correlation\n                matrices of the synthetic and test datasets, keyed by\n                \"cosine_dist_corr_matrix\". A lower distance indicates better preservation\n                of feature relationships.\n    \"\"\"\n    return {\n        \"cosine_dist_corr_matrix\": correlation_matrix_distance(\n            synth, test, metric=\"cosine\"\n        )\n    }\n</code></pre>"},{"location":"utils/#wgan_gp.utils.calc_utility_metrics","title":"<code>calc_utility_metrics(synth, x_train, x_test, y_test, y_train)</code>","text":"<p>Calculates utility metrics to assess how well synthetic data preserves the characteristics of real data for regression tasks.</p> <pre><code>This function trains a regression model on both real and synthetic datasets and then evaluates the performance of both models on a common test set.\nThe comparison of these performances indicates the utility of the synthetic data for maintaining the performance of machine learning models.\n\nArgs:\n    synth (torch.Tensor | np.ndarray): The synthetic data, where the last column is assumed to be the target variable.\n    x_train (torch.Tensor | np.ndarray): The training data features from the real dataset.\n    x_test (torch.Tensor | np.ndarray): The test data features from the real dataset.\n    y_test (torch.Tensor | np.ndarray): The test data labels from the real dataset.\n    y_train (torch.Tensor | np.ndarray): The training data labels from the real dataset.\n\nReturns:\n    tuple[dict, dict]: A tuple containing two dictionaries. The first dictionary contains the regression performance metrics achieved using real data,\n    and the second dictionary contains the regression performance metrics achieved using synthetic data.\n</code></pre> Source code in <code>wgan_gp/utils.py</code> <pre><code>def calc_utility_metrics(\n    synth: torch.Tensor | np.ndarray,\n    x_train: torch.Tensor | np.ndarray,\n    x_test: torch.Tensor | np.ndarray,\n    y_test: torch.Tensor | np.ndarray,\n    y_train: torch.Tensor | np.ndarray,\n):\n    \"\"\"\n    Calculates utility metrics to assess how well synthetic data preserves the characteristics of real data for regression tasks.\n\n        This function trains a regression model on both real and synthetic datasets and then evaluates the performance of both models on a common test set.\n        The comparison of these performances indicates the utility of the synthetic data for maintaining the performance of machine learning models.\n\n        Args:\n            synth (torch.Tensor | np.ndarray): The synthetic data, where the last column is assumed to be the target variable.\n            x_train (torch.Tensor | np.ndarray): The training data features from the real dataset.\n            x_test (torch.Tensor | np.ndarray): The test data features from the real dataset.\n            y_test (torch.Tensor | np.ndarray): The test data labels from the real dataset.\n            y_train (torch.Tensor | np.ndarray): The training data labels from the real dataset.\n\n        Returns:\n            tuple[dict, dict]: A tuple containing two dictionaries. The first dictionary contains the regression performance metrics achieved using real data,\n            and the second dictionary contains the regression performance metrics achieved using synthetic data.\n    \"\"\"\n\n    if isinstance(synth, np.ndarray):\n        synth = torch.from_numpy(synth)\n\n    if isinstance(x_train, np.ndarray):\n        x_train = torch.from_numpy(x_train)\n\n    if isinstance(x_test, np.ndarray):\n        x_test = torch.from_numpy(x_test)\n\n    if isinstance(y_test, np.ndarray):\n        y_test = torch.from_numpy(y_test)\n\n    if isinstance(y_train, np.ndarray):\n        y_train = torch.from_numpy(y_train)\n\n    metrics_real = compute_regression_performance(\n        x_train,\n        y_train,\n        x_test,\n        y_test,\n    )\n\n    synth_X, synth_y = synth[:, :-1], synth[:, -1]\n\n    metrics_synth = compute_regression_performance(\n        synth_X,\n        synth_y,\n        x_test,\n        y_test,\n    )\n    return convert_result(metrics_real), convert_result(metrics_synth)\n</code></pre>"},{"location":"utils/#wgan_gp.utils.compute_regression_performance","title":"<code>compute_regression_performance(X, y, X_test, y_test)</code>","text":"<p>Computes and compares the performance of regression models on real data.</p> <p>This method trains a set of regressors on the provided training data and evaluates their performance on a separate test set. It calculates Mean Absolute Percentage Error (MAPE), Root Mean Squared Error (RMSE), and R-squared (R2) to quantify the accuracy and goodness-of-fit of each regressor. This is done to establish a baseline performance of the models on the real data, which can then be compared with the performance of models trained on synthetic data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>Training data features.</p> required <code>y</code> <p>Training data target.</p> required <code>X_test</code> <p>Test data features.</p> required <code>y_test</code> <p>Test data target.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the performance metrics for each regressor. The keys of the dictionary are the names of the regressors, and the values are dictionaries containing the MAPE, RMSE, and R2 scores. For example: {'XGBRegressor': {'mape': 0.123, 'rmse': 4.56, 'r2': 0.789}}</p> Source code in <code>wgan_gp/utils.py</code> <pre><code>def compute_regression_performance(X, y, X_test, y_test):\n    \"\"\"\n    Computes and compares the performance of regression models on real data.\n\n    This method trains a set of regressors on the provided training data and evaluates their performance on a separate test set. It calculates Mean Absolute Percentage Error (MAPE), Root Mean Squared Error (RMSE), and R-squared (R2) to quantify the accuracy and goodness-of-fit of each regressor. This is done to establish a baseline performance of the models on the real data, which can then be compared with the performance of models trained on synthetic data.\n\n    Args:\n        X: Training data features.\n        y: Training data target.\n        X_test: Test data features.\n        y_test: Test data target.\n\n    Returns:\n        dict: A dictionary containing the performance metrics for each regressor.\n            The keys of the dictionary are the names of the regressors, and the\n            values are dictionaries containing the MAPE, RMSE, and R2 scores.\n            For example:\n            {'XGBRegressor': {'mape': 0.123, 'rmse': 4.56, 'r2': 0.789}}\n    \"\"\"\n    regressors = [XGBRegressor()]\n    result = {i.__class__.__name__: {} for i in regressors}\n\n    for regressor in regressors:\n        regressor.fit(X, y)\n        predictions = regressor.predict(X_test)\n\n        mape = mean_absolute_percentage_error(y_test, predictions)\n        rmse = root_mean_squared_error(y_test, predictions)\n        r2 = r2_score(y_test, predictions)\n        local_result = {\n            \"mape\": mape,\n            \"rmse\": rmse,\n            \"r2\": r2,\n        }\n        result[regressor.__class__.__name__] = local_result\n    return result\n</code></pre>"},{"location":"utils/#wgan_gp.utils.convert_result","title":"<code>convert_result(results)</code>","text":"<p>Converts the model results into a flattened dictionary format suitable for evaluating the performance of synthetic data generation.</p> <pre><code>This method takes a dictionary of model results, calculates the mean and\nstandard deviation of the metrics for each model, and then flattens the\nresults into a single row dictionary. This flattened structure facilitates\ncomparison and analysis of different synthetic data generation approaches\nby providing a consolidated view of their performance metrics.\n\nArgs:\n  results: A dictionary where keys are model names and values are\n    dictionaries of metrics (e.g., mape, rmse, r2).\n\nReturns:\n  A list containing a single dictionary. The dictionary's keys are\n  constructed by concatenating the model name and metric name with mean/std\n  (e.g., 'model1_mape_mean', 'model1_rmse_std'), and the values are the\n  corresponding calculated values. This format is used to create a single-row\n  representation of the aggregated metrics, which is useful for comparing\n  different synthetic data generation models.\n</code></pre> Source code in <code>wgan_gp/utils.py</code> <pre><code>def convert_result(results):\n    \"\"\"\n    Converts the model results into a flattened dictionary format suitable for evaluating the performance of synthetic data generation.\n\n        This method takes a dictionary of model results, calculates the mean and\n        standard deviation of the metrics for each model, and then flattens the\n        results into a single row dictionary. This flattened structure facilitates\n        comparison and analysis of different synthetic data generation approaches\n        by providing a consolidated view of their performance metrics.\n\n        Args:\n          results: A dictionary where keys are model names and values are\n            dictionaries of metrics (e.g., mape, rmse, r2).\n\n        Returns:\n          A list containing a single dictionary. The dictionary's keys are\n          constructed by concatenating the model name and metric name with mean/std\n          (e.g., 'model1_mape_mean', 'model1_rmse_std'), and the values are the\n          corresponding calculated values. This format is used to create a single-row\n          representation of the aggregated metrics, which is useful for comparing\n          different synthetic data generation models.\n    \"\"\"\n    df = pd.DataFrame(\n        [\n            {\n                \"model\": model,\n                \"mape\": metrics[\"mape\"],\n                \"rmse\": metrics[\"rmse\"],\n                \"r2\": metrics[\"r2\"],\n            }\n            for model, metrics in results.items()\n        ]\n    )\n    df_agg = df.groupby(\"model\").agg([\"mean\", \"std\"])\n    df_agg.columns = [\"_\".join(col) for col in df_agg.columns]\n\n    df_flat = df_agg.stack().rename_axis([\"model\", \"metric\"]).reset_index()\n    df_flat[\"colname\"] = df_flat[\"model\"] + \"_\" + df_flat[\"metric\"]\n    df_single_row = df_flat.set_index(\"colname\")[0].to_frame().T\n    return df_single_row.to_dict(orient=\"records\")\n</code></pre>"},{"location":"utils/#wgan_gp.utils.correlation_matrix_distance","title":"<code>correlation_matrix_distance(df1, df2, metric='frobenius')</code>","text":"<p>Compute the distance between the correlation structures of two datasets.</p> <pre><code>This function quantifies the dissimilarity in the relationships between variables\nin two datasets by comparing their correlation matrices. This is useful for\nassessing how well synthetic data replicates the correlational behavior of real data.\n\nArgs:\n    df1 (pandas.DataFrame): The first DataFrame.\n    df2 (pandas.DataFrame): The second DataFrame.\n    metric (str, optional): The distance metric to use.\n        Options are 'frobenius', 'euclidean', 'cosine', and 'spectral'.\n        Defaults to 'frobenius'.\n\nReturns:\n    float: The distance between the correlation matrices of the two DataFrames.\n</code></pre> Source code in <code>wgan_gp/utils.py</code> <pre><code>def correlation_matrix_distance(\n    df1: pd.DataFrame, df2: pd.DataFrame, metric: str = \"frobenius\"\n) -&gt; float:\n    \"\"\"\n    Compute the distance between the correlation structures of two datasets.\n\n        This function quantifies the dissimilarity in the relationships between variables\n        in two datasets by comparing their correlation matrices. This is useful for\n        assessing how well synthetic data replicates the correlational behavior of real data.\n\n        Args:\n            df1 (pandas.DataFrame): The first DataFrame.\n            df2 (pandas.DataFrame): The second DataFrame.\n            metric (str, optional): The distance metric to use.\n                Options are 'frobenius', 'euclidean', 'cosine', and 'spectral'.\n                Defaults to 'frobenius'.\n\n        Returns:\n            float: The distance between the correlation matrices of the two DataFrames.\n    \"\"\"\n    # Ensure same columns and order\n    common_cols = df1.columns.intersection(df2.columns)\n    df1 = df1[common_cols].dropna()\n    df2 = df2[common_cols].dropna()\n\n    # Truncate to equal length if needed\n    min_len = min(len(df1), len(df2))\n    df1 = df1.iloc[:min_len]\n    df2 = df2.iloc[:min_len]\n\n    # Compute correlation matrices\n    corr1 = df1.corr().values\n    corr2 = df2.corr().values\n\n    # Distance computation\n    if metric == \"frobenius\":\n        return np.linalg.norm(corr1 - corr2, ord=\"fro\")\n    elif metric == \"euclidean\":\n        return np.linalg.norm((corr1 - corr2).ravel())\n    elif metric == \"cosine\":\n        return cosine(corr1.ravel(), corr2.ravel())\n    elif metric == \"spectral\":\n        return np.linalg.norm(np.linalg.eigvalsh(corr1 - corr2), ord=2)\n    else:\n        raise ValueError(\n            f\"Unsupported metric '{metric}'. Choose from: 'frobenius', 'euclidean', 'cosine', 'spectral'.\"\n        )\n</code></pre>"},{"location":"utils/#wgan_gp.utils.create_joint","title":"<code>create_joint(plot=False, sample_size=1000, arc_size=1000)</code>","text":"<p>Generates a joint dataset consisting of two Gaussian blobs and a noisy arc.</p> <pre><code>This function creates a synthetic dataset composed of two distinct Gaussian\nclusters and a semi-circular arc with added noise. This type of dataset\nis useful for evaluating the ability of the GAN to capture different data\ndistributions and complex shapes, which is important for generating\nhigh-quality synthetic data that preserves the characteristics of real-world\ndatasets. The optional plotting functionality allows for visual inspection\nof the generated data and its kernel density estimation (KDE).\n\nArgs:\n  plot (bool): If True, plots the generated data and its KDE. Defaults to False.\n  sample_size (int): The number of samples to generate for each Gaussian blob. Defaults to 1000.\n  arc_size (int): The number of points to generate for the arc. Defaults to 1000.\n\nReturns:\n  torch.Tensor: A tensor containing all generated samples (Gaussian blobs and arc).\n</code></pre> Source code in <code>wgan_gp/utils.py</code> <pre><code>def create_joint(plot=False, sample_size=1000, arc_size=1000):\n    \"\"\"\n    Generates a joint dataset consisting of two Gaussian blobs and a noisy arc.\n\n        This function creates a synthetic dataset composed of two distinct Gaussian\n        clusters and a semi-circular arc with added noise. This type of dataset\n        is useful for evaluating the ability of the GAN to capture different data\n        distributions and complex shapes, which is important for generating\n        high-quality synthetic data that preserves the characteristics of real-world\n        datasets. The optional plotting functionality allows for visual inspection\n        of the generated data and its kernel density estimation (KDE).\n\n        Args:\n          plot (bool): If True, plots the generated data and its KDE. Defaults to False.\n          sample_size (int): The number of samples to generate for each Gaussian blob. Defaults to 1000.\n          arc_size (int): The number of points to generate for the arc. Defaults to 1000.\n\n        Returns:\n          torch.Tensor: A tensor containing all generated samples (Gaussian blobs and arc).\n    \"\"\"\n    # Generate two Gaussian blobs\n    mean1 = [3, 3]\n    mean2 = [-5, -5]\n    cov = [[0.2, 0], [0, 0.2]]\n    samples1 = np.random.multivariate_normal(mean1, cov, size=sample_size)\n    samples2 = np.random.multivariate_normal(mean2, cov, size=sample_size)\n\n    # Create a perturbed arc (half-circle)\n    theta = np.linspace(0, 2 * np.pi, arc_size)\n    arc_x = np.cos(theta)\n    arc_y = np.sin(theta)\n    arc = np.stack([arc_x, arc_y], axis=1)\n\n    # Translate and add noise to arc\n    arc = 5.5 * arc  # scale to match distance\n    arc[:, 0] -= 1  # center between the two blobs\n    arc[:, 1] -= 1  # center between the two blobs\n    arc += np.random.normal(scale=0.2, size=arc.shape)\n\n    # Combine all points\n    all_samples = np.vstack([samples1, samples2, arc])\n\n    if plot:\n        # Perform KDE\n        kde = gaussian_kde(all_samples.T)\n        x_grid, y_grid = np.mgrid[-10:10:100j, -10:10:100j]\n        positions = np.vstack([x_grid.ravel(), y_grid.ravel()])\n        density = kde(positions).reshape(x_grid.shape)\n\n        # Plot\n        plt.figure(figsize=(8, 6))\n        plt.contourf(x_grid, y_grid, density, levels=50, cmap=\"viridis\")\n        plt.scatter(all_samples[:, 0], all_samples[:, 1], s=5, color=\"white\", alpha=0.5)\n        plt.title(\"KDE of 2 Gaussians + Noisy Arc\")\n        plt.xlabel(\"x\")\n        plt.ylabel(\"y\")\n        plt.axis(\"equal\")\n        plt.grid(True)\n        plt.show()\n\n    return torch.tensor(all_samples, dtype=torch.float)\n</code></pre>"},{"location":"utils/#wgan_gp.utils.create_variates","title":"<code>create_variates(X, y=None, sample_frac=0.3, sample_number=100)</code>","text":"<p>Creates a set of data samples (variates) from the input tensor <code>X</code>.</p> <pre><code>This function generates multiple subsets (variates) of the original data `X`.\nThese variates are created by randomly sampling a fraction of the original\ndata points.  Creating these diverse samples is crucial for downstream tasks\nthat benefit from ensemble methods or require multiple perspectives on the data.\n\nArgs:\n    X (torch.Tensor): The input tensor representing the original dataset.\n    y (torch.Tensor, optional): An optional tensor of labels, not used in the sampling process. Defaults to None.\n    sample_frac (float, optional): The fraction of the original data size to include in each variate. Defaults to 0.3.\n    sample_number (int, optional): The number of variates (samples) to generate. Defaults to 100.\n\nReturns:\n    list: A list of tensors, where each tensor is a variate (sample) drawn from `X`.\n</code></pre> Source code in <code>wgan_gp/utils.py</code> <pre><code>def create_variates(\n    X: torch.Tensor, y: torch.Tensor = None, sample_frac=0.3, sample_number=100\n):\n    \"\"\"\n    Creates a set of data samples (variates) from the input tensor `X`.\n\n        This function generates multiple subsets (variates) of the original data `X`.\n        These variates are created by randomly sampling a fraction of the original\n        data points.  Creating these diverse samples is crucial for downstream tasks\n        that benefit from ensemble methods or require multiple perspectives on the data.\n\n        Args:\n            X (torch.Tensor): The input tensor representing the original dataset.\n            y (torch.Tensor, optional): An optional tensor of labels, not used in the sampling process. Defaults to None.\n            sample_frac (float, optional): The fraction of the original data size to include in each variate. Defaults to 0.3.\n            sample_number (int, optional): The number of variates (samples) to generate. Defaults to 100.\n\n        Returns:\n            list: A list of tensors, where each tensor is a variate (sample) drawn from `X`.\n    \"\"\"\n    assert sample_frac &lt; 1\n    total_size = X.size(0)\n    n_samples = int(total_size * sample_frac)\n    return [sample_from_Xy_tensors(X, n_samples) for _ in range(sample_number)]\n</code></pre>"},{"location":"utils/#wgan_gp.utils.estimate_marginal_js","title":"<code>estimate_marginal_js(df1, df2, epsilon=1e-10)</code>","text":"<p>Estimates the marginal Jensen-Shannon divergence between two dataframes for common columns.</p> <pre><code>This function quantifies the dissimilarity between the distributions of individual columns\nin two dataframes. It computes histograms for each common column, normalizes them into\nprobability distributions, and then calculates the Jensen-Shannon divergence between\nthese distributions. This helps assess how well the synthetic data (df2) replicates\nthe distribution of features in the real data (df1).\n\nArgs:\n    df1: The first dataframe, representing the real data.\n    df2: The second dataframe, representing the synthetic data.\n    epsilon: A small value added to histogram counts to avoid zero-probabilities (default: 1e-10).\n\nReturns:\n    dict: A dictionary where keys are the common column names and values are the\n        corresponding Jensen-Shannon divergence scores. Lower scores indicate greater similarity\n        between the distributions of the real and synthetic data for that column.\n</code></pre> Source code in <code>wgan_gp/utils.py</code> <pre><code>def estimate_marginal_js(df1, df2, epsilon=1e-10):\n    \"\"\"\n    Estimates the marginal Jensen-Shannon divergence between two dataframes for common columns.\n\n        This function quantifies the dissimilarity between the distributions of individual columns\n        in two dataframes. It computes histograms for each common column, normalizes them into\n        probability distributions, and then calculates the Jensen-Shannon divergence between\n        these distributions. This helps assess how well the synthetic data (df2) replicates\n        the distribution of features in the real data (df1).\n\n        Args:\n            df1: The first dataframe, representing the real data.\n            df2: The second dataframe, representing the synthetic data.\n            epsilon: A small value added to histogram counts to avoid zero-probabilities (default: 1e-10).\n\n        Returns:\n            dict: A dictionary where keys are the common column names and values are the\n                corresponding Jensen-Shannon divergence scores. Lower scores indicate greater similarity\n                between the distributions of the real and synthetic data for that column.\n    \"\"\"\n    js_results = {}\n\n    common_cols = df1.columns.intersection(df2.columns)\n\n    for col in common_cols:\n        x = df1[col].dropna().values\n        y = df2[col].dropna().values\n\n        # Histogram counts (not density, we'll normalize manually)\n        p_hist, _ = np.histogram(x)\n        q_hist, _ = np.histogram(y)\n\n        # Add small epsilon to avoid zero-probabilities\n        p_hist = p_hist + epsilon\n        q_hist = q_hist + epsilon\n\n        # Normalize to get valid distributions\n        p_prob = p_hist / p_hist.sum()\n        q_prob = q_hist / q_hist.sum()\n\n        # JS divergence is the square of the JS distance from scipy\n        js_div = jensenshannon(p_prob, q_prob, base=2) ** 2\n        js_results[col] = js_div\n\n    return js_results\n</code></pre>"},{"location":"utils/#wgan_gp.utils.get_real_data","title":"<code>get_real_data(batch_size)</code>","text":"<p>Generates a batch of real data points from a mixture of two Gaussians.</p> <pre><code>The data is sampled from two Gaussian distributions: one centered at [3, 3]\nand the other at [-5, -5], both with a standard deviation of 1.0. The\nmethod randomly chooses between these two distributions for each data point\nin the batch. This serves as the real data distribution that the generator\nwill attempt to mimic.\n\nArgs:\n    batch_size (int): The number of data points to generate in the batch.\n\nReturns:\n    torch.Tensor: A tensor of shape (batch_size, 2) containing the generated\n        data points. The data type is torch.float.\n</code></pre> Source code in <code>wgan_gp/utils.py</code> <pre><code>def get_real_data(batch_size):\n    \"\"\"\n    Generates a batch of real data points from a mixture of two Gaussians.\n\n        The data is sampled from two Gaussian distributions: one centered at [3, 3]\n        and the other at [-5, -5], both with a standard deviation of 1.0. The\n        method randomly chooses between these two distributions for each data point\n        in the batch. This serves as the real data distribution that the generator\n        will attempt to mimic.\n\n        Args:\n            batch_size (int): The number of data points to generate in the batch.\n\n        Returns:\n            torch.Tensor: A tensor of shape (batch_size, 2) containing the generated\n                data points. The data type is torch.float.\n    \"\"\"\n    mix = np.random.choice(2, size=(batch_size,))\n    data = np.zeros((batch_size, 2))\n    data[mix == 0] = np.random.normal(loc=[3, 3], scale=1.0, size=(np.sum(mix == 0), 2))\n    data[mix == 1] = np.random.normal(\n        loc=[-5, -5], scale=1.0, size=(np.sum(mix == 1), 2)\n    )\n    return torch.tensor(data, dtype=torch.float)\n</code></pre>"},{"location":"utils/#wgan_gp.utils.random_sample","title":"<code>random_sample(arr, size=1)</code>","text":"<p>Generates a random sample from an array without replacement. This is useful for creating subsets of the original data to train and evaluate machine learning models, ensuring that the evaluation is performed on unseen data.</p> <pre><code>Args:\n    arr (np.array): The input array.\n    size (int): The number of samples to generate. Defaults to 1.\n\nReturns:\n    np.array: A new array containing the random sample.\n</code></pre> Source code in <code>wgan_gp/utils.py</code> <pre><code>def random_sample(arr: np.array, size: int = 1) -&gt; np.array:\n    \"\"\"\n    Generates a random sample from an array without replacement. This is useful for creating subsets of the original data to train and evaluate machine learning models, ensuring that the evaluation is performed on unseen data.\n\n        Args:\n            arr (np.array): The input array.\n            size (int): The number of samples to generate. Defaults to 1.\n\n        Returns:\n            np.array: A new array containing the random sample.\n    \"\"\"\n    return arr[np.random.choice(len(arr), size=size, replace=False)]\n</code></pre>"},{"location":"utils/#wgan_gp.utils.remove_anomalies_iqr","title":"<code>remove_anomalies_iqr(x, y)</code>","text":"<p>Removes outliers from the input DataFrame using the IQR method to improve the quality and stability of synthetic data generation.</p> <pre><code>This function calculates the first quartile (Q1), third quartile (Q3), and\ninterquartile range (IQR) of the input DataFrame `x`. It then identifies and\nremoves rows containing values outside the range of Q1 - 1.5 * IQR to\nQ3 + 1.5 * IQR. This step is crucial for ensuring that the generated synthetic\ndata is not skewed by extreme values present in the original dataset, leading\nto more representative and reliable synthetic samples.\n\nArgs:\n    x: The DataFrame to remove anomalies from.\n    y: The target DataFrame.\n\nReturns:\n    tuple: A tuple containing two DataFrames:\n        - The first DataFrame contains the data from `x` with anomalies removed.\n        - The second DataFrame contains the corresponding target values from `y`.\n</code></pre> Source code in <code>wgan_gp/utils.py</code> <pre><code>def remove_anomalies_iqr(x: pd.DataFrame, y: pd.DataFrame):\n    \"\"\"\n    Removes outliers from the input DataFrame using the IQR method to improve the quality and stability of synthetic data generation.\n\n        This function calculates the first quartile (Q1), third quartile (Q3), and\n        interquartile range (IQR) of the input DataFrame `x`. It then identifies and\n        removes rows containing values outside the range of Q1 - 1.5 * IQR to\n        Q3 + 1.5 * IQR. This step is crucial for ensuring that the generated synthetic\n        data is not skewed by extreme values present in the original dataset, leading\n        to more representative and reliable synthetic samples.\n\n        Args:\n            x: The DataFrame to remove anomalies from.\n            y: The target DataFrame.\n\n        Returns:\n            tuple: A tuple containing two DataFrames:\n                - The first DataFrame contains the data from `x` with anomalies removed.\n                - The second DataFrame contains the corresponding target values from `y`.\n    \"\"\"\n    Q1 = x.quantile(0.25)\n    Q3 = x.quantile(0.75)\n    IQR = Q3 - Q1\n\n    real_data_scaled_no_anomalies_indexes = x[\n        ~((x &lt; (Q1 - 1.5 * IQR)) | (x &gt; (Q3 + 1.5 * IQR))).any(axis=1)\n    ].index\n    real_data_scaled_no_anomalies = x.iloc[\n        real_data_scaled_no_anomalies_indexes, :\n    ].values\n    real_data_scaled_no_anomalies = real_data_scaled_no_anomalies.values\n    y = pd.DataFrame(y[real_data_scaled_no_anomalies.index], columns=[\"target\"])\n\n    # real_data_scaled_no_anomalies[\"target\"] = y[real_data_scaled_no_anomalies.index]\n    # real_data_scaled_no_anomalies.to_csv(\"california_scaled_no_anomalies.csv\", index=False)\n    return real_data_scaled_no_anomalies, y\n</code></pre>"},{"location":"utils/#wgan_gp.utils.remove_inf","title":"<code>remove_inf(diagram, replacement=None)</code>","text":"<p>Replaces infinite death times in a persistence diagram with a finite value to ensure compatibility with downstream analysis and visualization tools. This is necessary because many tools cannot handle infinite values, which can arise in persistence diagrams when topological features persist indefinitely.</p> <pre><code>Args:\n    diagram (np.ndarray): A persistence diagram represented as a NumPy array, where each row corresponds to a topological feature and contains birth and death times.\n    replacement (float, optional): The value to use as a replacement for infinite death times. If None, it defaults to 1.1 times the maximum finite death time in the diagram.\n\nReturns:\n    np.ndarray: A copy of the input persistence diagram with infinite death times replaced by a finite value.\n</code></pre> Source code in <code>wgan_gp/utils.py</code> <pre><code>def remove_inf(diagram, replacement=None):\n    \"\"\"\n    Replaces infinite death times in a persistence diagram with a finite value to ensure compatibility with downstream analysis and visualization tools. This is necessary because many tools cannot handle infinite values, which can arise in persistence diagrams when topological features persist indefinitely.\n\n        Args:\n            diagram (np.ndarray): A persistence diagram represented as a NumPy array, where each row corresponds to a topological feature and contains birth and death times.\n            replacement (float, optional): The value to use as a replacement for infinite death times. If None, it defaults to 1.1 times the maximum finite death time in the diagram.\n\n        Returns:\n            np.ndarray: A copy of the input persistence diagram with infinite death times replaced by a finite value.\n    \"\"\"\n    if len(diagram) == 0:\n        return diagram\n    finite_deaths = diagram[np.isfinite(diagram[:, 1]), 1]\n    if len(finite_deaths) == 0:\n        # All death times are inf \u2014 choose arbitrary value\n        finite_max = 1.0\n    else:\n        finite_max = np.max(finite_deaths)\n    if replacement is None:\n        replacement = 1.1 * finite_max\n    diagram = diagram.copy()\n    diagram[np.isinf(diagram[:, 1]), 1] = replacement\n    return diagram\n</code></pre>"},{"location":"utils/#wgan_gp.utils.sample_from_Xy_tensors","title":"<code>sample_from_Xy_tensors(X, n_samples)</code>","text":"<p>Samples a subset of rows from a given data tensor to create a smaller, representative dataset. This is useful for efficiently training and evaluating models on a manageable portion of the data while preserving its key characteristics.</p> <pre><code>Args:\n    X (torch.Tensor): The input data tensor to sample from.\n    n_samples (int): The number of samples to extract.\n\nReturns:\n    torch.Tensor: A tensor containing the sampled rows from the input tensor X, representing a subset of the original data.\n</code></pre> Source code in <code>wgan_gp/utils.py</code> <pre><code>def sample_from_Xy_tensors(X: torch.Tensor, n_samples):\n    \"\"\"\n    Samples a subset of rows from a given data tensor to create a smaller, representative dataset. This is useful for efficiently training and evaluating models on a manageable portion of the data while preserving its key characteristics.\n\n        Args:\n            X (torch.Tensor): The input data tensor to sample from.\n            n_samples (int): The number of samples to extract.\n\n        Returns:\n            torch.Tensor: A tensor containing the sampled rows from the input tensor X, representing a subset of the original data.\n    \"\"\"\n    indices = torch.randperm(X.size(0))\n    indices_trunc = indices[:n_samples]\n    X_sampled_tensor = X[indices_trunc]\n    # y_sampled_tensor = y[indices_trunc[:n_samples]]\n    # return X_sampled_tensor, y_sampled_tensor\n    return X_sampled_tensor\n</code></pre>"},{"location":"utils/#wgan_gp.utils.topological_distance","title":"<code>topological_distance(X, Y, maxdim=2)</code>","text":"<p>Compute the topological distance between two point clouds using persistent homology.</p> <pre><code>This method leverages persistent homology and the Wasserstein distance to\nquantify the dissimilarity between the topological features of two point clouds,\nspecifically focusing on 0-dimensional (H0) and 1-dimensional (H1) features.\nBy comparing the persistence diagrams, it provides a measure of how different\nthe underlying shapes and connectivity patterns are between the datasets.\nThis is crucial for evaluating how well the generated data captures the\nessential topological characteristics of the real data.\n\nArgs:\n    X (numpy.ndarray): The first point cloud.\n    Y (numpy.ndarray): The second point cloud.\n    maxdim (int, optional): The maximum dimension to compute persistent homology for.\n        Defaults to 2.\n\nReturns:\n    tuple: A tuple containing the Wasserstein distances for H0 and H1.\n    The first element is the Wasserstein distance between the H0\n    persistence diagrams. The second element is the Wasserstein\n    distance between the H1 persistence diagrams, or None if the\n    persistence diagrams do not contain H1.\n</code></pre> Source code in <code>wgan_gp/utils.py</code> <pre><code>def topological_distance(X, Y, maxdim=2):\n    \"\"\"\n    Compute the topological distance between two point clouds using persistent homology.\n\n        This method leverages persistent homology and the Wasserstein distance to\n        quantify the dissimilarity between the topological features of two point clouds,\n        specifically focusing on 0-dimensional (H0) and 1-dimensional (H1) features.\n        By comparing the persistence diagrams, it provides a measure of how different\n        the underlying shapes and connectivity patterns are between the datasets.\n        This is crucial for evaluating how well the generated data captures the\n        essential topological characteristics of the real data.\n\n        Args:\n            X (numpy.ndarray): The first point cloud.\n            Y (numpy.ndarray): The second point cloud.\n            maxdim (int, optional): The maximum dimension to compute persistent homology for.\n                Defaults to 2.\n\n        Returns:\n            tuple: A tuple containing the Wasserstein distances for H0 and H1.\n            The first element is the Wasserstein distance between the H0\n            persistence diagrams. The second element is the Wasserstein\n            distance between the H1 persistence diagrams, or None if the\n            persistence diagrams do not contain H1.\n    \"\"\"\n    dgms_X = ripser(X, maxdim=maxdim)[\"dgms\"]\n    dgms_Y = ripser(Y, maxdim=maxdim)[\"dgms\"]\n    h0 = wasserstein(dgms_X[0], dgms_Y[0])\n    h1 = wasserstein(dgms_X[1], dgms_Y[1]) if len(dgms_X) &gt; 1 else None\n    return h0, h1\n</code></pre>"},{"location":"wgan_gp/","title":"WGAN-GP Module","text":""},{"location":"wgan_gp/#overview","title":"Overview","text":"<p>This module implements a Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP) tailored for synthetic tabular data generation. It includes components for defining datasets, generator and discriminator models, meta-feature extraction using PyTorch, and custom training procedures. The module provides tools for incorporating Meta-Feature Statistics (MFS) into the training loop, enabling the preservation of statistical properties such as correlation, covariance, eigenvalues, mean, and variance to improve the fidelity and utility of the generated synthetic data.</p>"},{"location":"wgan_gp/#purpose","title":"Purpose","text":"<p>The primary purpose of this module is to generate high-quality synthetic tabular datasets that closely mimic real-world data distributions while preserving essential statistical characteristics. It provides a framework for training GANs with gradient penalty to stabilize training and incorporates MFS loss components to ensure the synthetic data maintains the same meta-feature distributions as the original data. This approach is particularly valuable for:</p> <ul> <li>Data Augmentation: Creating additional training samples while preserving data characteristics</li> <li>Privacy Preservation: Generating synthetic datasets that maintain utility without exposing sensitive information  </li> <li>Research and Development: Providing representative datasets when real data access is limited or restricted</li> <li>Model Testing: Creating controlled datasets with known statistical properties for algorithm validation</li> </ul>"},{"location":"wgan_gp/#key-components","title":"Key Components","text":"<p>The WGAN-GP module consists of several interconnected components:</p>"},{"location":"wgan_gp/#core-architecture","title":"Core Architecture","text":"<ul> <li>Generator: Residual network-based architecture that transforms random noise into synthetic data samples</li> <li>Discriminator: Multi-layer network that distinguishes between real and synthetic data using Wasserstein distance</li> <li>Gradient Penalty: Regularization technique that enforces Lipschitz constraint for stable training</li> </ul>"},{"location":"wgan_gp/#meta-feature-statistics-integration","title":"Meta-Feature Statistics Integration","text":"<ul> <li>MFE PyTorch Implementation: Custom PyTorch implementation of meta-feature extraction</li> <li>Statistical Preservation: Maintains correlation matrices, covariance structures, eigenvalues, and distributional properties</li> <li>Wasserstein Distance Matching: Uses optimal transport theory to align meta-feature distributions between real and synthetic data</li> </ul>"},{"location":"wgan_gp/#training-infrastructure","title":"Training Infrastructure","text":"<ul> <li>Dual Training Modes: Support for both vanilla WGAN-GP and MFS-enhanced training</li> <li>Flexible Loss Functions: Configurable weighting between adversarial loss and meta-feature preservation</li> <li>Experiment Tracking: Built-in Aim integration for comprehensive training monitoring</li> </ul> <p>This module enables researchers and practitioners to generate synthetic data that not only looks realistic but also preserves the underlying statistical structure necessary for downstream machine learning tasks.</p>"},{"location":"wgan_gp/models/","title":"Models Module","text":"<p>This module defines the neural network architectures for the WGAN-GP implementation.</p>"},{"location":"wgan_gp/models/#architecture-overview","title":"Architecture Overview","text":"<p>The models are designed for tabular data generation with residual connections to improve training stability and gradient flow.</p>"},{"location":"wgan_gp/models/#classes","title":"Classes","text":""},{"location":"wgan_gp/models/#residual","title":"Residual","text":"<p>Building block layer that applies linear transformation, batch normalization, and ReLU activation with residual connections.</p>"},{"location":"wgan_gp/models/#generator","title":"Generator","text":"<p>Residual network-based generator that transforms random noise into synthetic tabular data samples.</p>"},{"location":"wgan_gp/models/#discriminator","title":"Discriminator","text":"<p>Multi-layer discriminator network that evaluates the quality of generated samples using Wasserstein distance.</p>"},{"location":"wgan_gp/models/#key-features","title":"Key Features","text":"<ul> <li>Residual Connections: Improved gradient flow and training stability</li> <li>Configurable Architecture: Flexible layer dimensions for different dataset sizes</li> <li>Batch Normalization: Stabilized training dynamics</li> <li>Leaky ReLU Activations: Better gradient propagation in discriminator</li> </ul>"},{"location":"wgan_gp/models/#wgan_gp.models","title":"<code>wgan_gp.models</code>","text":""},{"location":"wgan_gp/models/#wgan_gp.models.Discriminator","title":"<code>Discriminator</code>","text":"<p>               Bases: <code>Module</code></p> <p>Discriminator.</p> Source code in <code>wgan_gp/models.py</code> <pre><code>class Discriminator(nn.Module):\n    \"\"\"\n    Discriminator.\n    \"\"\"\n\n    def __init__(self, data_dim, discriminator_dim):\n        \"\"\"\n        Initializes the Discriminator network.\n\n        The discriminator is a sequential neural network designed to differentiate\n        between real and synthetic data samples. It is built using linear layers\n        followed by LeakyReLU activations to introduce non-linearity, enabling it\n        to learn complex data distributions.\n\n        Args:\n            data_dim (int): The dimension of the input data. This defines the\n                number of features in each data sample that the discriminator will\n                evaluate.\n            discriminator_dim (list): A list of integers specifying the number of\n                neurons in each hidden layer of the discriminator network. This\n                determines the capacity and complexity of the discriminator.\n\n        Attributes:\n            data_dim (int): Stores the dimension of the input data for use in the\n                network's forward pass.\n            seq (nn.Sequential): An nn.Sequential container that holds the\n                discriminator's layers. This allows for easy forward propagation\n                through the entire network.\n        \"\"\"\n        super(Discriminator, self).__init__()\n        seq = []\n        self.data_dim = data_dim\n\n        dim = data_dim\n        for item in list(discriminator_dim):\n            # seq += [nn.Linear(dim, item), nn.LeakyReLU(0.2), nn.Dropout(0.3)]\n            seq += [nn.Linear(dim, item), nn.LeakyReLU(0.2)]\n            dim = item\n\n        seq += [nn.Linear(dim, 1)]\n        self.seq = nn.Sequential(*seq)\n\n    def forward(self, input_):\n        \"\"\"\n        Applies the discriminator network to the input data.\n\n        The discriminator aims to distinguish between real and synthetic data samples. This method\n        reshapes the input and processes it through a sequential model to output a classification\n        score, indicating the likelihood of the input being real. This is a crucial step in\n        training the GAN, allowing the discriminator to learn the characteristics of real data\n        and guide the generator's learning process.\n\n        Args:\n            input_ (torch.Tensor): The input data to be evaluated by the discriminator.\n\n        Returns:\n            Output (torch.Tensor): The discriminator's output, representing the classification score  for the input data.\n        \"\"\"\n        return self.seq(input_.view(-1, self.data_dim))\n</code></pre>"},{"location":"wgan_gp/models/#wgan_gp.models.Discriminator.__init__","title":"<code>__init__(data_dim, discriminator_dim)</code>","text":"<p>Initializes the Discriminator network.</p> <p>The discriminator is a sequential neural network designed to differentiate between real and synthetic data samples. It is built using linear layers followed by LeakyReLU activations to introduce non-linearity, enabling it to learn complex data distributions.</p> <p>Parameters:</p> Name Type Description Default <code>data_dim</code> <code>int</code> <p>The dimension of the input data. This defines the number of features in each data sample that the discriminator will evaluate.</p> required <code>discriminator_dim</code> <code>list</code> <p>A list of integers specifying the number of neurons in each hidden layer of the discriminator network. This determines the capacity and complexity of the discriminator.</p> required <p>Attributes:</p> Name Type Description <code>data_dim</code> <code>int</code> <p>Stores the dimension of the input data for use in the network's forward pass.</p> <code>seq</code> <code>Sequential</code> <p>An nn.Sequential container that holds the discriminator's layers. This allows for easy forward propagation through the entire network.</p> Source code in <code>wgan_gp/models.py</code> <pre><code>def __init__(self, data_dim, discriminator_dim):\n    \"\"\"\n    Initializes the Discriminator network.\n\n    The discriminator is a sequential neural network designed to differentiate\n    between real and synthetic data samples. It is built using linear layers\n    followed by LeakyReLU activations to introduce non-linearity, enabling it\n    to learn complex data distributions.\n\n    Args:\n        data_dim (int): The dimension of the input data. This defines the\n            number of features in each data sample that the discriminator will\n            evaluate.\n        discriminator_dim (list): A list of integers specifying the number of\n            neurons in each hidden layer of the discriminator network. This\n            determines the capacity and complexity of the discriminator.\n\n    Attributes:\n        data_dim (int): Stores the dimension of the input data for use in the\n            network's forward pass.\n        seq (nn.Sequential): An nn.Sequential container that holds the\n            discriminator's layers. This allows for easy forward propagation\n            through the entire network.\n    \"\"\"\n    super(Discriminator, self).__init__()\n    seq = []\n    self.data_dim = data_dim\n\n    dim = data_dim\n    for item in list(discriminator_dim):\n        # seq += [nn.Linear(dim, item), nn.LeakyReLU(0.2), nn.Dropout(0.3)]\n        seq += [nn.Linear(dim, item), nn.LeakyReLU(0.2)]\n        dim = item\n\n    seq += [nn.Linear(dim, 1)]\n    self.seq = nn.Sequential(*seq)\n</code></pre>"},{"location":"wgan_gp/models/#wgan_gp.models.Discriminator.forward","title":"<code>forward(input_)</code>","text":"<p>Applies the discriminator network to the input data.</p> <p>The discriminator aims to distinguish between real and synthetic data samples. This method reshapes the input and processes it through a sequential model to output a classification score, indicating the likelihood of the input being real. This is a crucial step in training the GAN, allowing the discriminator to learn the characteristics of real data and guide the generator's learning process.</p> <p>Parameters:</p> Name Type Description Default <code>input_</code> <code>Tensor</code> <p>The input data to be evaluated by the discriminator.</p> required <p>Returns:</p> Name Type Description <code>Output</code> <code>Tensor</code> <p>The discriminator's output, representing the classification score  for the input data.</p> Source code in <code>wgan_gp/models.py</code> <pre><code>def forward(self, input_):\n    \"\"\"\n    Applies the discriminator network to the input data.\n\n    The discriminator aims to distinguish between real and synthetic data samples. This method\n    reshapes the input and processes it through a sequential model to output a classification\n    score, indicating the likelihood of the input being real. This is a crucial step in\n    training the GAN, allowing the discriminator to learn the characteristics of real data\n    and guide the generator's learning process.\n\n    Args:\n        input_ (torch.Tensor): The input data to be evaluated by the discriminator.\n\n    Returns:\n        Output (torch.Tensor): The discriminator's output, representing the classification score  for the input data.\n    \"\"\"\n    return self.seq(input_.view(-1, self.data_dim))\n</code></pre>"},{"location":"wgan_gp/models/#wgan_gp.models.Generator","title":"<code>Generator</code>","text":"<p>               Bases: <code>Module</code></p> <p>Generator.</p> Source code in <code>wgan_gp/models.py</code> <pre><code>class Generator(nn.Module):\n    \"\"\"\n    Generator.\n    \"\"\"\n\n    def __init__(self, embedding_dim, generator_dim, data_dim):\n        \"\"\"\n        Initializes a Generator object.\n\n        The generator network consists of a series of residual blocks followed by a linear layer.\n        It takes a latent vector as input and outputs a data sample that mimics the real data\n        distribution. This architecture allows the generator to learn complex data patterns and\n        generate synthetic samples with high fidelity.\n\n        Args:\n            embedding_dim (int): Dimension of the latent embedding. This determines the size\n                of the input noise vector.\n            generator_dim (list): A list of dimensions for the residual blocks. Each value\n                specifies the output dimension of a residual block, progressively increasing\n                the feature space.\n            data_dim (int): Dimension of the output data. This should match the dimensionality\n                of the real data samples that the generator is trying to emulate.\n\n        Attributes:\n            latent_dim (int): Dimension of the latent embedding.\n            seq (nn.Sequential): A sequential container holding the generator network layers.\n        \"\"\"\n        super(Generator, self).__init__()\n        self.latent_dim = embedding_dim\n        dim = embedding_dim\n        seq = []\n        for item in list(generator_dim):\n            seq += [Residual(dim, item)]\n            dim += item\n        seq.append(nn.Linear(dim, data_dim))\n        self.seq = nn.Sequential(*seq)\n\n    def forward(self, input_):\n        \"\"\"\n        Transforms the input data using a sequence of layers.\n\n        This process aims to generate synthetic data that mirrors the characteristics of real data,\n        making it suitable for tasks requiring realistic datasets.\n\n        Args:\n            input_: The input data to be transformed. This could be real data or random noise.\n\n        Returns:\n            The transformed data after passing through the generator's layers. This represents\n            the synthetic data generated by the network.\n        \"\"\"\n        data = self.seq(input_)\n        return data\n\n    def sample_latent(self, num_samples):\n        \"\"\"\n        Samples latent vectors from a standard normal distribution.\n\n        These vectors serve as the initial input to the generator, guiding the creation of\n        synthetic samples. Sampling from a standard normal distribution ensures diversity in\n        the generated output, allowing the GAN to explore different regions of the data space.\n\n        Args:\n            num_samples (int): The number of latent vectors to sample.\n\n        Returns:\n             Output (torch.Tensor): A tensor of shape (num_samples, latent_dim) containing the sampled\n                latent vectors.\n        \"\"\"\n        return torch.randn((num_samples, self.latent_dim))\n</code></pre>"},{"location":"wgan_gp/models/#wgan_gp.models.Generator.__init__","title":"<code>__init__(embedding_dim, generator_dim, data_dim)</code>","text":"<p>Initializes a Generator object.</p> <p>The generator network consists of a series of residual blocks followed by a linear layer. It takes a latent vector as input and outputs a data sample that mimics the real data distribution. This architecture allows the generator to learn complex data patterns and generate synthetic samples with high fidelity.</p> <p>Parameters:</p> Name Type Description Default <code>embedding_dim</code> <code>int</code> <p>Dimension of the latent embedding. This determines the size of the input noise vector.</p> required <code>generator_dim</code> <code>list</code> <p>A list of dimensions for the residual blocks. Each value specifies the output dimension of a residual block, progressively increasing the feature space.</p> required <code>data_dim</code> <code>int</code> <p>Dimension of the output data. This should match the dimensionality of the real data samples that the generator is trying to emulate.</p> required <p>Attributes:</p> Name Type Description <code>latent_dim</code> <code>int</code> <p>Dimension of the latent embedding.</p> <code>seq</code> <code>Sequential</code> <p>A sequential container holding the generator network layers.</p> Source code in <code>wgan_gp/models.py</code> <pre><code>def __init__(self, embedding_dim, generator_dim, data_dim):\n    \"\"\"\n    Initializes a Generator object.\n\n    The generator network consists of a series of residual blocks followed by a linear layer.\n    It takes a latent vector as input and outputs a data sample that mimics the real data\n    distribution. This architecture allows the generator to learn complex data patterns and\n    generate synthetic samples with high fidelity.\n\n    Args:\n        embedding_dim (int): Dimension of the latent embedding. This determines the size\n            of the input noise vector.\n        generator_dim (list): A list of dimensions for the residual blocks. Each value\n            specifies the output dimension of a residual block, progressively increasing\n            the feature space.\n        data_dim (int): Dimension of the output data. This should match the dimensionality\n            of the real data samples that the generator is trying to emulate.\n\n    Attributes:\n        latent_dim (int): Dimension of the latent embedding.\n        seq (nn.Sequential): A sequential container holding the generator network layers.\n    \"\"\"\n    super(Generator, self).__init__()\n    self.latent_dim = embedding_dim\n    dim = embedding_dim\n    seq = []\n    for item in list(generator_dim):\n        seq += [Residual(dim, item)]\n        dim += item\n    seq.append(nn.Linear(dim, data_dim))\n    self.seq = nn.Sequential(*seq)\n</code></pre>"},{"location":"wgan_gp/models/#wgan_gp.models.Generator.forward","title":"<code>forward(input_)</code>","text":"<p>Transforms the input data using a sequence of layers.</p> <p>This process aims to generate synthetic data that mirrors the characteristics of real data, making it suitable for tasks requiring realistic datasets.</p> <p>Parameters:</p> Name Type Description Default <code>input_</code> <p>The input data to be transformed. This could be real data or random noise.</p> required <p>Returns:</p> Type Description <p>The transformed data after passing through the generator's layers. This represents</p> <p>the synthetic data generated by the network.</p> Source code in <code>wgan_gp/models.py</code> <pre><code>def forward(self, input_):\n    \"\"\"\n    Transforms the input data using a sequence of layers.\n\n    This process aims to generate synthetic data that mirrors the characteristics of real data,\n    making it suitable for tasks requiring realistic datasets.\n\n    Args:\n        input_: The input data to be transformed. This could be real data or random noise.\n\n    Returns:\n        The transformed data after passing through the generator's layers. This represents\n        the synthetic data generated by the network.\n    \"\"\"\n    data = self.seq(input_)\n    return data\n</code></pre>"},{"location":"wgan_gp/models/#wgan_gp.models.Generator.sample_latent","title":"<code>sample_latent(num_samples)</code>","text":"<p>Samples latent vectors from a standard normal distribution.</p> <p>These vectors serve as the initial input to the generator, guiding the creation of synthetic samples. Sampling from a standard normal distribution ensures diversity in the generated output, allowing the GAN to explore different regions of the data space.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>The number of latent vectors to sample.</p> required <p>Returns:</p> Name Type Description <code>Output</code> <code>Tensor</code> <p>A tensor of shape (num_samples, latent_dim) containing the sampled latent vectors.</p> Source code in <code>wgan_gp/models.py</code> <pre><code>def sample_latent(self, num_samples):\n    \"\"\"\n    Samples latent vectors from a standard normal distribution.\n\n    These vectors serve as the initial input to the generator, guiding the creation of\n    synthetic samples. Sampling from a standard normal distribution ensures diversity in\n    the generated output, allowing the GAN to explore different regions of the data space.\n\n    Args:\n        num_samples (int): The number of latent vectors to sample.\n\n    Returns:\n         Output (torch.Tensor): A tensor of shape (num_samples, latent_dim) containing the sampled\n            latent vectors.\n    \"\"\"\n    return torch.randn((num_samples, self.latent_dim))\n</code></pre>"},{"location":"wgan_gp/models/#wgan_gp.models.Residual","title":"<code>Residual</code>","text":"<p>               Bases: <code>Module</code></p> <p>Residual layer.</p> Source code in <code>wgan_gp/models.py</code> <pre><code>class Residual(nn.Module):\n    \"\"\"\n    Residual layer.\n    \"\"\"\n\n    def __init__(self, i, o):\n        \"\"\"\n        Initializes a Residual block for feature transformation within the GAN.\n\n        This block applies a linear transformation, batch normalization, and ReLU activation\n        to refine features during the generation or discrimination process. It helps the network\n        learn more complex representations by introducing non-linearities and stabilizing the\n        learning process.\n\n        Args:\n            i (int): The input feature size.\n            o (int): The output feature size.\n\n        Attributes:\n            fc (nn.Linear): A linear layer that transforms the input from size 'i' to size 'o'.\n            bn (nn.BatchNorm1d): A batch normalization layer applied to the output of the\n                linear layer, stabilizing the activations.\n            relu (nn.ReLU): A ReLU activation function applied after batch normalization,\n                introducing non-linearity.\n        \"\"\"\n        super(Residual, self).__init__()\n        self.fc = nn.Linear(i, o)\n        self.bn = nn.BatchNorm1d(o)\n        self.relu = nn.ReLU()\n\n    def forward(self, input_):\n        \"\"\"\n        Applies a residual connection to the input after passing it through a fully connected\n        layer, batch normalization (commented out), and ReLU activation.\n\n        This process enhances the model's ability to capture intricate data patterns by\n        concatenating the processed input with the original input, thus preserving original\n        data characteristics while introducing non-linear transformations.\n\n        Args:\n            input_ (torch.Tensor): The input tensor to the residual layer.\n\n        Returns:\n            Output (torch.Tensor): The concatenated tensor of the processed input and the original input.\n        \"\"\"\n        out = self.fc(input_)\n        # out = self.bn(out)\n        out = self.relu(out)\n        return torch.cat([out, input_], dim=1)\n</code></pre>"},{"location":"wgan_gp/models/#wgan_gp.models.Residual.__init__","title":"<code>__init__(i, o)</code>","text":"<p>Initializes a Residual block for feature transformation within the GAN.</p> <p>This block applies a linear transformation, batch normalization, and ReLU activation to refine features during the generation or discrimination process. It helps the network learn more complex representations by introducing non-linearities and stabilizing the learning process.</p> <p>Parameters:</p> Name Type Description Default <code>i</code> <code>int</code> <p>The input feature size.</p> required <code>o</code> <code>int</code> <p>The output feature size.</p> required <p>Attributes:</p> Name Type Description <code>fc</code> <code>Linear</code> <p>A linear layer that transforms the input from size 'i' to size 'o'.</p> <code>bn</code> <code>BatchNorm1d</code> <p>A batch normalization layer applied to the output of the linear layer, stabilizing the activations.</p> <code>relu</code> <code>ReLU</code> <p>A ReLU activation function applied after batch normalization, introducing non-linearity.</p> Source code in <code>wgan_gp/models.py</code> <pre><code>def __init__(self, i, o):\n    \"\"\"\n    Initializes a Residual block for feature transformation within the GAN.\n\n    This block applies a linear transformation, batch normalization, and ReLU activation\n    to refine features during the generation or discrimination process. It helps the network\n    learn more complex representations by introducing non-linearities and stabilizing the\n    learning process.\n\n    Args:\n        i (int): The input feature size.\n        o (int): The output feature size.\n\n    Attributes:\n        fc (nn.Linear): A linear layer that transforms the input from size 'i' to size 'o'.\n        bn (nn.BatchNorm1d): A batch normalization layer applied to the output of the\n            linear layer, stabilizing the activations.\n        relu (nn.ReLU): A ReLU activation function applied after batch normalization,\n            introducing non-linearity.\n    \"\"\"\n    super(Residual, self).__init__()\n    self.fc = nn.Linear(i, o)\n    self.bn = nn.BatchNorm1d(o)\n    self.relu = nn.ReLU()\n</code></pre>"},{"location":"wgan_gp/models/#wgan_gp.models.Residual.forward","title":"<code>forward(input_)</code>","text":"<p>Applies a residual connection to the input after passing it through a fully connected layer, batch normalization (commented out), and ReLU activation.</p> <p>This process enhances the model's ability to capture intricate data patterns by concatenating the processed input with the original input, thus preserving original data characteristics while introducing non-linear transformations.</p> <p>Parameters:</p> Name Type Description Default <code>input_</code> <code>Tensor</code> <p>The input tensor to the residual layer.</p> required <p>Returns:</p> Name Type Description <code>Output</code> <code>Tensor</code> <p>The concatenated tensor of the processed input and the original input.</p> Source code in <code>wgan_gp/models.py</code> <pre><code>def forward(self, input_):\n    \"\"\"\n    Applies a residual connection to the input after passing it through a fully connected\n    layer, batch normalization (commented out), and ReLU activation.\n\n    This process enhances the model's ability to capture intricate data patterns by\n    concatenating the processed input with the original input, thus preserving original\n    data characteristics while introducing non-linear transformations.\n\n    Args:\n        input_ (torch.Tensor): The input tensor to the residual layer.\n\n    Returns:\n        Output (torch.Tensor): The concatenated tensor of the processed input and the original input.\n    \"\"\"\n    out = self.fc(input_)\n    # out = self.bn(out)\n    out = self.relu(out)\n    return torch.cat([out, input_], dim=1)\n</code></pre>"},{"location":"wgan_gp/pymfe_to_torch/","title":"PyMFE to Torch Module","text":"<p>This module provides a PyTorch implementation of Meta-Feature Extraction (MFE) for statistical analysis of tabular data.</p>"},{"location":"wgan_gp/pymfe_to_torch/#overview","title":"Overview","text":"<p>The PyMFE to Torch module converts meta-feature extraction operations into PyTorch tensors, enabling differentiable computation of statistical properties during GAN training. This is essential for the Meta-Feature Statistics (MFS) preservation component of the WGAN-GP implementation.</p>"},{"location":"wgan_gp/pymfe_to_torch/#key-features","title":"Key Features","text":""},{"location":"wgan_gp/pymfe_to_torch/#statistical-meta-features","title":"Statistical Meta-Features","text":"<ul> <li>Correlation: Pearson correlation coefficients between features</li> <li>Covariance: Covariance matrix computation</li> <li>Eigenvalues: Principal component eigenvalues for dimensionality analysis</li> <li>Distributional Statistics: Mean, variance, standard deviation, range, min, max</li> <li>Advanced Statistics: Skewness, kurtosis, interquartile range, sparsity</li> </ul>"},{"location":"wgan_gp/pymfe_to_torch/#pytorch-integration","title":"PyTorch Integration","text":"<ul> <li>Differentiable Operations: All computations maintain gradient flow</li> <li>GPU Acceleration: CUDA-compatible tensor operations</li> <li>Batch Processing: Efficient computation over data batches</li> <li>Device Management: Automatic device placement for tensors</li> </ul>"},{"location":"wgan_gp/pymfe_to_torch/#mfetotorch-class","title":"MFEToTorch Class","text":"<p>The main class that provides: - Feature method mapping for easy access to statistical functions - Torch-native implementations of traditional meta-feature extraction - Integration with the training loop for real-time MFS computation - Support for subset feature selection for targeted preservation</p>"},{"location":"wgan_gp/pymfe_to_torch/#usage-in-training","title":"Usage in Training","text":"<p>This module is crucial for the MFS-enhanced WGAN-GP training, where it: 1. Computes meta-features for real data variates 2. Calculates corresponding features for generated synthetic data 3. Enables Wasserstein distance computation between feature distributions 4. Provides gradients for generator optimization</p>"},{"location":"wgan_gp/pymfe_to_torch/#wgan_gp.pymfe_to_torch","title":"<code>wgan_gp.pymfe_to_torch</code>","text":""},{"location":"wgan_gp/pymfe_to_torch/#wgan_gp.pymfe_to_torch.MFEToTorch","title":"<code>MFEToTorch</code>","text":"<p>A class to compute meta-features using PyTorch.</p> <p>This class provides methods to calculate various meta-features for a given dataset using PyTorch tensors. It includes functionalities for computing statistical measures, correlation, covariance, and other properties of the data.</p> <p>Meta-Feature Statistics (MFS) Available:</p> Feature Name Method Description <code>cor</code> <code>ft_cor_torch</code> Correlation matrix (absolute values of lower triangle) <code>cov</code> <code>ft_cov_torch</code> Covariance matrix (absolute values of lower triangle) <code>eigenvalues</code> <code>ft_eigenvals</code> Eigenvalues of the covariance matrix <code>iq_range</code> <code>ft_iq_range</code> Interquartile range (Q3 - Q1) <code>gravity</code> <code>ft_gravity_torch</code> Distance between majority and minority class centers <code>kurtosis</code> <code>ft_kurtosis</code> Fourth moment about the mean (tailedness) <code>skewness</code> <code>ft_skewness</code> Third moment about the mean (asymmetry) <code>mad</code> <code>ft_mad</code> Median Absolute Deviation <code>max</code> <code>ft_max</code> Maximum values along dimension 0 <code>min</code> <code>ft_min</code> Minimum values along dimension 0 <code>mean</code> <code>ft_mean</code> Mean values along dimension 0 <code>median</code> <code>ft_median</code> Median values along dimension 0 <code>range</code> <code>ft_range</code> Range (max - min) along dimension 0 <code>sd</code> <code>ft_std</code> Standard deviation along dimension 0 <code>var</code> <code>ft_var</code> Variance along dimension 0 <code>sparsity</code> <code>ft_sparsity</code> Feature sparsity (diversity of unique values) Usage <p>The class can be used to extract meta-features from datasets for GAN training with Meta-Feature Statistics preservation. Common subsets include:</p> <ul> <li>Basic statistics: <code>['mean', 'var', 'sd']</code></li> <li>Distribution properties: <code>['skewness', 'kurtosis', 'mad']</code></li> <li>Relationships: <code>['cor', 'cov', 'eigenvalues']</code></li> <li>Range measures: <code>['min', 'max', 'range', 'iq_range']</code></li> <li>Classification features: <code>['gravity']</code> (requires target variable)</li> </ul> <p>Attributes:</p> Name Type Description <code>device</code> <code>device</code> <p>Device for computation (default: 'cpu')</p> Source code in <code>wgan_gp/pymfe_to_torch.py</code> <pre><code>class MFEToTorch:\n    \"\"\"\n    A class to compute meta-features using PyTorch.\n\n    This class provides methods to calculate various meta-features for a given\n    dataset using PyTorch tensors. It includes functionalities for computing\n    statistical measures, correlation, covariance, and other properties of the\n    data.\n\n    Meta-Feature Statistics (MFS) Available:\n\n    | Feature Name | Method | Description |\n    |--------------|--------|-------------|\n    | `cor` | `ft_cor_torch` | Correlation matrix (absolute values of lower triangle) |\n    | `cov` | `ft_cov_torch` | Covariance matrix (absolute values of lower triangle) |\n    | `eigenvalues` | `ft_eigenvals` | Eigenvalues of the covariance matrix |\n    | `iq_range` | `ft_iq_range` | Interquartile range (Q3 - Q1) |\n    | `gravity` | `ft_gravity_torch` | Distance between majority and minority class centers |\n    | `kurtosis` | `ft_kurtosis` | Fourth moment about the mean (tailedness) |\n    | `skewness` | `ft_skewness` | Third moment about the mean (asymmetry) |\n    | `mad` | `ft_mad` | Median Absolute Deviation |\n    | `max` | `ft_max` | Maximum values along dimension 0 |\n    | `min` | `ft_min` | Minimum values along dimension 0 |\n    | `mean` | `ft_mean` | Mean values along dimension 0 |\n    | `median` | `ft_median` | Median values along dimension 0 |\n    | `range` | `ft_range` | Range (max - min) along dimension 0 |\n    | `sd` | `ft_std` | Standard deviation along dimension 0 |\n    | `var` | `ft_var` | Variance along dimension 0 |\n    | `sparsity` | `ft_sparsity` | Feature sparsity (diversity of unique values) |\n\n    Usage:\n        The class can be used to extract meta-features from datasets for GAN training\n        with Meta-Feature Statistics preservation. Common subsets include:\n\n        - Basic statistics: `['mean', 'var', 'sd']`\n        - Distribution properties: `['skewness', 'kurtosis', 'mad']`\n        - Relationships: `['cor', 'cov', 'eigenvalues']`\n        - Range measures: `['min', 'max', 'range', 'iq_range']`\n        - Classification features: `['gravity']` (requires target variable)\n\n    Attributes:\n        device (torch.device): Device for computation (default: 'cpu')\n    \"\"\"\n\n    device = torch.device(\"cpu\")\n\n    @property\n    def feature_methods(self):\n        \"\"\"\n        Returns a dictionary that maps feature names to their corresponding extraction methods.\n\n        This mapping is essential for calculating a comprehensive set of statistical\n        properties on both real and synthetic datasets. These features are then\n        used to evaluate the quality and utility of the generated synthetic data\n        by comparing them against the features of the real data.\n\n        Returns:\n            dict: A dictionary where keys are feature names (strings) and\n                values are the corresponding feature extraction methods.\n                See the class docstring for a complete table of available features.\n        \"\"\"\n        return {\n            \"cor\": self.ft_cor_torch,\n            \"cov\": self.ft_cov_torch,\n            \"eigenvalues\": self.ft_eigenvals,\n            \"iq_range\": self.ft_iq_range,\n            \"gravity\": self.ft_gravity_torch,\n            \"kurtosis\": self.ft_kurtosis,\n            \"skewness\": self.ft_skewness,\n            \"mad\": self.ft_mad,\n            \"max\": self.ft_max,\n            \"min\": self.ft_min,\n            \"mean\": self.ft_mean,\n            \"median\": self.ft_median,\n            \"range\": self.ft_range,\n            \"sd\": self.ft_std,\n            \"var\": self.ft_var,\n            \"sparsity\": self.ft_sparsity,\n        }\n\n    @staticmethod\n    def ft_gravity_torch(\n        N: torch.Tensor,\n        y: torch.Tensor,\n        norm_ord: Union[int, float] = 2,\n        classes: Optional[torch.Tensor] = None,\n        class_freqs: Optional[torch.Tensor] = None,\n        cls_inds: Optional[torch.Tensor] = None,\n    ):\n        \"\"\"\n        Computes the gravity between the majority and minority classes.\n\n        This method calculates the distance between the mean feature vectors of the\n        majority and minority classes. This distance serves as a measure of class\n        separation in the feature space. By computing this \"gravity,\" the method\n        quantifies the dissimilarity between the most and least frequent classes,\n        providing insight into the dataset's class distribution and feature\n        representation. This information can be valuable for assessing the quality\n        and representativeness of generated synthetic data compared to real data.\n\n        Args:\n            N: Feature tensor of shape (num_instances, num_features).\n            y: Target tensor of shape (num_instances,).\n            norm_ord: Order of the norm to compute the distance (e.g., 2 for Euclidean). Defaults to 2.\n            classes: Optional tensor of unique class labels. If None, it's computed from `y`.\n            class_freqs: Optional tensor of class frequencies. If None, it's computed from `y`.\n            cls_inds: Optional list of indices for each class. If provided, it uses these indices to select instances.\n\n        Returns:\n            torch.Tensor: The gravity value, representing the distance between the class centers.\n        \"\"\"\n        if classes is None or class_freqs is None:\n            classes, class_freqs = torch.unique(y, return_counts=True)\n\n        ind_cls_maj = torch.argmax(class_freqs)\n        class_maj = classes[ind_cls_maj]\n\n        remaining_classes = torch.cat(\n            (classes[:ind_cls_maj], classes[ind_cls_maj + 1 :])\n        )\n        remaining_freqs = torch.cat(\n            (class_freqs[:ind_cls_maj], class_freqs[ind_cls_maj + 1 :])\n        )\n\n        ind_cls_min = torch.argmin(remaining_freqs)\n\n        if cls_inds is not None:\n            insts_cls_maj = N[cls_inds[ind_cls_maj]]\n            if ind_cls_min &gt;= ind_cls_maj:\n                ind_cls_min += 1\n            insts_cls_min = N[cls_inds[ind_cls_min]]\n        else:\n            class_min = remaining_classes[ind_cls_min]\n            insts_cls_maj = N[y == class_maj]\n            insts_cls_min = N[y == class_min]\n\n        center_maj = insts_cls_maj.mean(dim=0)\n        center_min = insts_cls_min.mean(dim=0)\n        gravity = torch.norm(center_maj - center_min, p=norm_ord)\n\n        return gravity\n\n    def change_device(self, device):\n        \"\"\"\n        Changes the device where computations will be performed.\n\n        Args:\n            device (str): The target device (e.g., 'cpu', 'cuda').\n\n        This method is crucial for ensuring that the model and data reside on the same device,\n        allowing for efficient computation and utilization of available hardware resources\n        during the synthetic data generation and evaluation processes.\n        \"\"\"\n        self.device = device\n\n    @staticmethod\n    def cov(tensor, rowvar=True, bias=False):\n        \"\"\"\n        Estimates the covariance matrix of a given tensor, crucial for understanding the statistical relationships within the data. This is a key step in evaluating how well the generated synthetic data captures the underlying dependencies present in the original data.\n\n                Args:\n                    tensor (torch.Tensor): Input data tensor.\n                    rowvar (bool, optional): If True (default), rows represent variables, with observations in the columns. If False, columns represent variables.\n                    bias (bool, optional): If False (default), then the normalization is by N-1. Otherwise, normalization is by N.\n\n                Returns:\n                    torch.Tensor: The covariance matrix of the input tensor.\n        \"\"\"\n        tensor = tensor if rowvar else tensor.transpose(-1, -2)\n        tensor = tensor - tensor.mean(dim=-1, keepdim=True)\n        factor = 1 / (tensor.shape[-1] - int(not bool(bias)))\n        return factor * tensor @ tensor.transpose(-1, -2).conj()\n\n    def corrcoef(self, tensor, rowvar=True):\n        \"\"\"\n        Calculates the Pearson product-moment correlation coefficients, normalizing the covariance matrix by the standard deviations to obtain correlation values. This provides a measure of the linear relationship between variables in the input tensor, which is useful for comparing real and synthetic data.\n\n        Args:\n            tensor (torch.Tensor): Input data tensor.\n            rowvar (bool, optional): If True (default), rows represent variables, with observations in the columns. Otherwise, columns represent variables.\n\n        Returns:\n            torch.Tensor: Pearson product-moment correlation coefficients matrix.\n        \"\"\"\n        covariance = self.cov(tensor, rowvar=rowvar)\n        variance = covariance.diagonal(0, -1, -2)\n        if variance.is_complex():\n            variance = variance.real\n        stddev = variance.sqrt()\n        covariance /= stddev.unsqueeze(-1)\n        covariance /= stddev.unsqueeze(-2)\n        if covariance.is_complex():\n            covariance.real.clip_(-1, 1)\n            covariance.imag.clip_(-1, 1)\n        else:\n            covariance.clip_(-1, 1)\n        return covariance\n\n    def ft_cor_torch(self, N: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Calculates the absolute values of the lower triangle elements of a correlation matrix to quantify feature dependencies.\n\n        This method computes the correlation matrix of the input tensor `N`,\n        extracts the elements from the lower triangle (excluding the diagonal),\n        and returns the absolute values of these elements. This is done to summarize the relationships between features,\n        which is useful for evaluating how well the synthetic data captures the dependencies present in the real data.\n        By focusing on the lower triangle and taking absolute values, the method efficiently provides a measure of feature interconnectedness,\n        ignoring self-correlations and directionality.\n\n        Args:\n            N: The input tensor for which to compute the correlation matrix.\n\n        Returns:\n            torch.Tensor: A tensor containing the absolute values of the elements\n                in the lower triangle of the correlation matrix.\n        \"\"\"\n        corr_mat = self.corrcoef(N, rowvar=False)\n        res_num_rows, _ = corr_mat.shape\n\n        tril_indices = torch.tril_indices(res_num_rows, res_num_rows, offset=-1)\n        inf_triang_vals = corr_mat[tril_indices[0], tril_indices[1]]\n\n        return torch.abs(inf_triang_vals)\n\n    def ft_cov_torch(\n        self,\n        N: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Calculates the absolute values of the lower triangular elements of the covariance matrix. This focuses on the relationships between variables, extracting the lower triangle to reduce redundancy and focusing on key covariance values. The absolute value ensures that the magnitude of the covariance is considered, regardless of the direction of the relationship.\n\n        Args:\n            N: Input tensor for covariance calculation.\n\n        Returns:\n            torch.Tensor: A tensor containing the absolute values of the lower triangular elements of the covariance matrix.\n        \"\"\"\n        cov_mat = self.cov(N, rowvar=False)\n\n        res_num_rows = cov_mat.shape[0]\n        tril_indices = torch.tril_indices(res_num_rows, res_num_rows, offset=-1)\n        inf_triang_vals = cov_mat[tril_indices[0], tril_indices[1]]\n\n        return torch.abs(inf_triang_vals)\n\n    def ft_eigenvals(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Computes the eigenvalues of the covariance matrix of the input tensor.\n\n        This function is crucial for assessing the diversity and information\n        content of the input data. By calculating the eigenvalues of the\n        covariance matrix, we gain insights into the principal components\n        and variance distribution within the data, which helps to ensure\n        the generated synthetic data retains the key statistical\n        characteristics of the original data.\n\n        Args:\n            x (torch.Tensor): The input tensor.\n\n        Returns:\n            torch.Tensor: The eigenvalues of the covariance matrix.\n        \"\"\"\n        # taking real part of first two eigenvals\n        centered = x - x.mean(dim=0, keepdim=True)\n        covs = self.cov(centered, rowvar=False)\n        return torch.linalg.eigvalsh(covs)\n\n    @staticmethod\n    def ft_iq_range(X: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Calculates the interquartile range (IQR) of a tensor along the first dimension.\n\n        The IQR is a measure of statistical dispersion, representing the difference between the 75th and 25th percentiles. This is useful for understanding the spread of the data, which helps to assess the utility of generated synthetic data by comparing its distribution to the real data.\n\n        Args:\n            X: The input tensor of shape [num_samples, num_features].\n\n        Returns:\n            The interquartile range of the input tensor, with shape [num_features]. This represents the spread of each feature across the samples.\n        \"\"\"\n        q75, q25 = torch.quantile(X, 0.75, dim=0), torch.quantile(X, 0.25, dim=0)\n        iqr = q75 - q25  # shape: [num_features]\n        return iqr\n\n    @staticmethod\n    def ft_kurtosis(x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Calculates the kurtosis of a tensor.\n\n        This function computes the kurtosis of the input tensor `x`, a statistical measure\n        describing the shape of the data's distribution, specifically its tailedness.\n        By calculating kurtosis, we can assess how well the generated data's distribution\n        matches that of the real data, ensuring the synthetic data retains similar statistical\n        properties. This is crucial for maintaining the utility of the generated data in downstream tasks.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: The kurtosis of the input tensor.\n        \"\"\"\n        mean = torch.mean(x)\n        diffs = x - mean\n        var = torch.mean(torch.pow(diffs, 2.0))\n        std = torch.pow(var, 0.5)\n        zscores = diffs / std\n        kurtoses = torch.mean(torch.pow(zscores, 4.0)) - 3.0\n        return kurtoses\n\n    @staticmethod\n    def ft_skewness(x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Computes the skewness of a tensor.\n\n        This function calculates the skewness of the input tensor, a key statistical\n        measure reflecting the asymmetry of the data distribution. Preserving this characteristic\n        is crucial when generating synthetic data to maintain the real data's statistical properties.\n\n        Args:\n            x (torch.Tensor): The input tensor.\n\n        Returns:\n            torch.Tensor: The skewness of the input tensor.\n        \"\"\"\n        mean = torch.mean(x)\n        diffs = x - mean\n        var = torch.mean(torch.pow(diffs, 2.0))\n        std = torch.pow(var, 0.5)\n        zscores = diffs / std\n        skews = torch.mean(torch.pow(zscores, 3.0))\n        return skews\n\n    @staticmethod\n    def ft_mad(x: torch.Tensor, factor: float = 1.4826) -&gt; torch.Tensor:\n        \"\"\"\n        Compute the Median Absolute Deviation (MAD) of a tensor.\n\n        The MAD is a robust measure of statistical dispersion, useful for\n        understanding the spread of data in both real and synthetic datasets.\n        It helps assess how well the generated data captures the variability\n        present in the original data.\n\n        Args:\n            x: The input tensor.\n            factor: A scaling factor to make the MAD an unbiased estimator of the\n                standard deviation for normal data. Default is 1.4826, which\n                applies when the data is normally distributed.\n\n        Returns:\n            torch.Tensor: The MAD of the input tensor.\n        \"\"\"\n        m = x.median(dim=0, keepdim=True).values\n        ama = torch.abs(x - m)\n        mama = ama.median(dim=0).values\n        return mama / (1 / factor)\n\n    @staticmethod\n    def ft_mean(N: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Computes the mean of a tensor along the first dimension to aggregate information across samples. This is useful for summarizing the central tendency of features in the generated or real data.\n\n        Args:\n            N (torch.Tensor): The input tensor.\n\n        Returns:\n            torch.Tensor: The mean of the input tensor along dimension 0.\n        \"\"\"\n        return N.mean(dim=0)\n\n    @staticmethod\n    def ft_max(N: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Finds the maximum value in a tensor along dimension 0. This is used to identify the most prominent features across a dataset, which is crucial for maintaining data utility in generated synthetic data.\n\n        Args:\n            N (torch.Tensor): The input tensor.\n\n        Returns:\n            torch.Tensor: A tensor containing the maximum values along dimension 0.\n        \"\"\"\n        return N.max(dim=0, keepdim=False).values\n\n    @staticmethod\n    def ft_median(N: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Calculates the median of a tensor along the first dimension. This is used to derive a representative central tendency of the data distribution, which is a crucial aspect of maintaining data utility in synthetic data generation.\n\n        Args:\n            N: The input tensor.\n\n        Returns:\n            torch.Tensor: A tensor containing the median values along the first dimension.\n        \"\"\"\n        return N.median(dim=0).values\n\n    @staticmethod\n    def ft_min(N: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Finds the minimum value of a tensor along dimension 0, which is useful for identifying the smallest values across different samples when comparing real and synthetic data distributions.\n\n        Args:\n            N (torch.Tensor): The input tensor.\n\n        Returns:\n            torch.Tensor: A tensor containing the minimum values along dimension 0. This represents the minimum feature values across the dataset, aiding in the comparison of feature ranges between real and synthetic datasets.\n        \"\"\"\n        return N.min(dim=0).values\n\n    @staticmethod\n    def ft_var(N):\n        \"\"\"\n        Calculates the variance of a tensor along dimension 0. This is a crucial step in assessing the statistical similarity between real and synthetic datasets generated by the GAN, ensuring that the generated data captures the variability present in the original data.\n\n        Args:\n            N (torch.Tensor): The input tensor.\n\n        Returns:\n            torch.Tensor: The variance of the input tensor along dimension 0.\n        \"\"\"\n        return torch.var(N, dim=0)\n\n    @staticmethod\n    def ft_std(N):\n        \"\"\"\n        Calculates the standard deviation of a tensor along the first dimension (dimension 0). This is used to understand the spread or dispersion of the generated synthetic data across different samples, ensuring the generated data maintains a similar statistical distribution to the real data.\n\n        Args:\n            N (torch.Tensor): The input tensor representing a batch of generated samples.\n\n        Returns:\n            torch.Tensor: The standard deviation of the input tensor along dimension 0, representing the standard deviation for each feature across the generated samples.\n        \"\"\"\n        return torch.std(N, dim=0)\n\n    @staticmethod\n    def ft_range(N: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Calculates the range of values (max - min) along the first dimension (dimension 0) of the input tensor. This is useful for understanding the spread or variability of the data along that dimension, which helps assess how well the generated data captures the characteristics of the original data.\n\n        Args:\n            N: The input tensor.\n\n        Returns:\n            torch.Tensor: A tensor containing the range (max - min) of values along dimension 0.\n        \"\"\"\n        return N.max(dim=0).values - N.min(dim=0).values\n\n    def ft_sparsity(self, N: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Calculates the feature sparsity of a given tensor.\n\n        This method computes the sparsity of each feature in the input tensor `N`.\n        Sparsity is defined as the ratio of the total number of instances to the\n        number of unique values for each feature, normalized to the range [0, 1].\n        This metric helps to assess the diversity of feature values, which is crucial\n        for generating synthetic data that accurately reflects the statistical\n        properties of the original dataset. By quantifying feature sparsity, we can\n        ensure that the generated data maintains a similar level of variability\n        as the real data, thereby preserving its utility for downstream tasks.\n\n        Args:\n            N (torch.Tensor): A tensor of shape (num_instances, num_features) representing the input data.\n\n        Returns:\n            torch.Tensor: A tensor of shape (num_features,) containing the sparsity\n            score for each feature, normalized to the range [0, 1]. The tensor is\n            moved to the device specified by `self.device`.\n        \"\"\"\n        ans = torch.tensor([attr.size(0) / torch.unique(attr).size(0) for attr in N.T])\n\n        num_inst = N.size(0)\n        norm_factor = 1.0 / (num_inst - 1.0)\n        result = (ans - 1.0) * norm_factor\n\n        return result.to(self.device)\n\n    def pad_only(self, tensor, target_len):\n        \"\"\"\n        Pads a tensor with zeros to a specified length, ensuring consistent input sizes for subsequent processing steps. This is particularly useful when dealing with variable-length sequences that need to be batched or processed by models requiring fixed-size inputs.\n\n        Args:\n            tensor (torch.Tensor): The input tensor to be padded.\n            target_len (int): The desired length of the padded tensor.\n\n        Returns:\n            torch.Tensor: The padded tensor, or the original tensor if its length is already greater than or equal to `target_len`.\n        \"\"\"\n        if tensor.shape[0] &lt; target_len:\n            padding = torch.zeros(target_len - tensor.shape[0]).to(self.device)\n            return torch.cat([tensor, padding])\n\n        return tensor\n\n    def get_mfs(self, X, y, subset=None):\n        \"\"\"\n        Computes a set of meta-features on the input data. These meta-features capture essential characteristics of the dataset, which is crucial for evaluating and ensuring the utility of synthetic data generated by GANs.\n\n        Args:\n            X (torch.Tensor): The input data tensor.\n            y (torch.Tensor, optional): The target variable tensor. Required if 'gravity' is in the subset.\n            subset (list of str, optional): A list of meta-feature names to compute. If None, defaults to ['mean', 'var'].\n\n        Returns:\n            torch.Tensor: A tensor containing the computed meta-features, padded to the maximum shape among the computed features and stacked into a single tensor. This allows for consistent representation and comparison of different meta-features.\n        \"\"\"\n        if subset is None:\n            subset = [\"mean\", \"var\"]\n\n        mfs = []\n        for name in subset:\n            if name not in self.feature_methods:\n                raise ValueError(f\"Unsupported meta-feature: '{name}'\")\n\n            if name == \"gravity\":\n                if y is None:\n                    raise ValueError(\"Meta-feature 'gravity' requires `y`.\")\n                res = self.feature_methods[name](X, y)\n                res = torch.tile(res, (X.shape[-1],))  # match dimensionality\n            else:\n                res = self.feature_methods[name](X)\n\n            mfs.append(res)\n        shapes = [i.shape.numel() for i in mfs]\n        mfs = [self.pad_only(mf, max(shapes)) for mf in mfs]\n        return torch.stack(mfs)\n\n    def test_me(self, subset=None):\n        \"\"\"\n        Compares meta-feature extraction using the `pymfe` package and the `MFEToTorch` class.\n\n        This method fetches the California Housing dataset, extracts meta-features using both `pymfe` and the `MFEToTorch` class, and then compares the results. This comparison helps validate the correctness and consistency of the meta-feature extraction process implemented in the `MFEToTorch` class, ensuring that it aligns with established meta-feature extraction tools.\n\n        Args:\n            subset (list, optional): A list of meta-features to extract. If None, defaults to [\"mean\", \"var\"].\n\n        Returns:\n            pandas.DataFrame: A DataFrame containing the meta-features extracted by both `pymfe` and `MFEToTorch`, along with any discrepancies between the two.\n        \"\"\"\n        if subset is None:\n            subset = [\"mean\", \"var\"]\n\n        from sklearn.datasets import fetch_california_housing\n\n        bunch = fetch_california_housing(as_frame=True)\n        X, y = bunch.data, bunch.target\n        print(f\"Init data shape: {X.shape} + {y.shape}\")\n\n        mfe = MFE(groups=\"statistical\", summary=None)\n        mfe.fit(X.values, y.values)\n        ft = mfe.extract()\n\n        pymfe = pd.DataFrame(\n            map(lambda x: [x], ft[1]), index=ft[0], columns=[\"pymfe\"]\n        ).dropna()\n\n        X_tensor = torch.tensor(X.values)\n        y_tensor = torch.tensor(y)\n\n        mfs = self.get_mfs(X_tensor, y_tensor, subset).numpy()\n        mfs_df = pd.DataFrame({\"torch_mfs\": list(mfs)})\n\n        mfs_df.index = subset\n        # mfs_df = mfs_df.reindex(self.mfs_available)\n\n        res = pymfe.merge(mfs_df, left_index=True, right_index=True, how=\"outer\")\n\n        def round_element(val, decimals=2):\n            if isinstance(val, list):\n                return [round(x, decimals) for x in val]\n            elif isinstance(val, np.ndarray):\n                return np.round(val, decimals)\n            return round(val, decimals)\n\n        res = res.map(lambda x: round_element(x, 5)).dropna()\n\n        print(res)\n</code></pre>"},{"location":"wgan_gp/pymfe_to_torch/#wgan_gp.pymfe_to_torch.MFEToTorch.feature_methods","title":"<code>feature_methods</code>  <code>property</code>","text":"<p>Returns a dictionary that maps feature names to their corresponding extraction methods.</p> <p>This mapping is essential for calculating a comprehensive set of statistical properties on both real and synthetic datasets. These features are then used to evaluate the quality and utility of the generated synthetic data by comparing them against the features of the real data.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary where keys are feature names (strings) and values are the corresponding feature extraction methods. See the class docstring for a complete table of available features.</p>"},{"location":"wgan_gp/pymfe_to_torch/#wgan_gp.pymfe_to_torch.MFEToTorch.change_device","title":"<code>change_device(device)</code>","text":"<p>Changes the device where computations will be performed.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>The target device (e.g., 'cpu', 'cuda').</p> required <p>This method is crucial for ensuring that the model and data reside on the same device, allowing for efficient computation and utilization of available hardware resources during the synthetic data generation and evaluation processes.</p> Source code in <code>wgan_gp/pymfe_to_torch.py</code> <pre><code>def change_device(self, device):\n    \"\"\"\n    Changes the device where computations will be performed.\n\n    Args:\n        device (str): The target device (e.g., 'cpu', 'cuda').\n\n    This method is crucial for ensuring that the model and data reside on the same device,\n    allowing for efficient computation and utilization of available hardware resources\n    during the synthetic data generation and evaluation processes.\n    \"\"\"\n    self.device = device\n</code></pre>"},{"location":"wgan_gp/pymfe_to_torch/#wgan_gp.pymfe_to_torch.MFEToTorch.corrcoef","title":"<code>corrcoef(tensor, rowvar=True)</code>","text":"<p>Calculates the Pearson product-moment correlation coefficients, normalizing the covariance matrix by the standard deviations to obtain correlation values. This provides a measure of the linear relationship between variables in the input tensor, which is useful for comparing real and synthetic data.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Input data tensor.</p> required <code>rowvar</code> <code>bool</code> <p>If True (default), rows represent variables, with observations in the columns. Otherwise, columns represent variables.</p> <code>True</code> <p>Returns:</p> Type Description <p>torch.Tensor: Pearson product-moment correlation coefficients matrix.</p> Source code in <code>wgan_gp/pymfe_to_torch.py</code> <pre><code>def corrcoef(self, tensor, rowvar=True):\n    \"\"\"\n    Calculates the Pearson product-moment correlation coefficients, normalizing the covariance matrix by the standard deviations to obtain correlation values. This provides a measure of the linear relationship between variables in the input tensor, which is useful for comparing real and synthetic data.\n\n    Args:\n        tensor (torch.Tensor): Input data tensor.\n        rowvar (bool, optional): If True (default), rows represent variables, with observations in the columns. Otherwise, columns represent variables.\n\n    Returns:\n        torch.Tensor: Pearson product-moment correlation coefficients matrix.\n    \"\"\"\n    covariance = self.cov(tensor, rowvar=rowvar)\n    variance = covariance.diagonal(0, -1, -2)\n    if variance.is_complex():\n        variance = variance.real\n    stddev = variance.sqrt()\n    covariance /= stddev.unsqueeze(-1)\n    covariance /= stddev.unsqueeze(-2)\n    if covariance.is_complex():\n        covariance.real.clip_(-1, 1)\n        covariance.imag.clip_(-1, 1)\n    else:\n        covariance.clip_(-1, 1)\n    return covariance\n</code></pre>"},{"location":"wgan_gp/pymfe_to_torch/#wgan_gp.pymfe_to_torch.MFEToTorch.cov","title":"<code>cov(tensor, rowvar=True, bias=False)</code>  <code>staticmethod</code>","text":"<p>Estimates the covariance matrix of a given tensor, crucial for understanding the statistical relationships within the data. This is a key step in evaluating how well the generated synthetic data captures the underlying dependencies present in the original data.</p> <pre><code>    Args:\n        tensor (torch.Tensor): Input data tensor.\n        rowvar (bool, optional): If True (default), rows represent variables, with observations in the columns. If False, columns represent variables.\n        bias (bool, optional): If False (default), then the normalization is by N-1. Otherwise, normalization is by N.\n\n    Returns:\n        torch.Tensor: The covariance matrix of the input tensor.\n</code></pre> Source code in <code>wgan_gp/pymfe_to_torch.py</code> <pre><code>@staticmethod\ndef cov(tensor, rowvar=True, bias=False):\n    \"\"\"\n    Estimates the covariance matrix of a given tensor, crucial for understanding the statistical relationships within the data. This is a key step in evaluating how well the generated synthetic data captures the underlying dependencies present in the original data.\n\n            Args:\n                tensor (torch.Tensor): Input data tensor.\n                rowvar (bool, optional): If True (default), rows represent variables, with observations in the columns. If False, columns represent variables.\n                bias (bool, optional): If False (default), then the normalization is by N-1. Otherwise, normalization is by N.\n\n            Returns:\n                torch.Tensor: The covariance matrix of the input tensor.\n    \"\"\"\n    tensor = tensor if rowvar else tensor.transpose(-1, -2)\n    tensor = tensor - tensor.mean(dim=-1, keepdim=True)\n    factor = 1 / (tensor.shape[-1] - int(not bool(bias)))\n    return factor * tensor @ tensor.transpose(-1, -2).conj()\n</code></pre>"},{"location":"wgan_gp/pymfe_to_torch/#wgan_gp.pymfe_to_torch.MFEToTorch.ft_cor_torch","title":"<code>ft_cor_torch(N)</code>","text":"<p>Calculates the absolute values of the lower triangle elements of a correlation matrix to quantify feature dependencies.</p> <p>This method computes the correlation matrix of the input tensor <code>N</code>, extracts the elements from the lower triangle (excluding the diagonal), and returns the absolute values of these elements. This is done to summarize the relationships between features, which is useful for evaluating how well the synthetic data captures the dependencies present in the real data. By focusing on the lower triangle and taking absolute values, the method efficiently provides a measure of feature interconnectedness, ignoring self-correlations and directionality.</p> <p>Parameters:</p> Name Type Description Default <code>N</code> <code>Tensor</code> <p>The input tensor for which to compute the correlation matrix.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the absolute values of the elements in the lower triangle of the correlation matrix.</p> Source code in <code>wgan_gp/pymfe_to_torch.py</code> <pre><code>def ft_cor_torch(self, N: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Calculates the absolute values of the lower triangle elements of a correlation matrix to quantify feature dependencies.\n\n    This method computes the correlation matrix of the input tensor `N`,\n    extracts the elements from the lower triangle (excluding the diagonal),\n    and returns the absolute values of these elements. This is done to summarize the relationships between features,\n    which is useful for evaluating how well the synthetic data captures the dependencies present in the real data.\n    By focusing on the lower triangle and taking absolute values, the method efficiently provides a measure of feature interconnectedness,\n    ignoring self-correlations and directionality.\n\n    Args:\n        N: The input tensor for which to compute the correlation matrix.\n\n    Returns:\n        torch.Tensor: A tensor containing the absolute values of the elements\n            in the lower triangle of the correlation matrix.\n    \"\"\"\n    corr_mat = self.corrcoef(N, rowvar=False)\n    res_num_rows, _ = corr_mat.shape\n\n    tril_indices = torch.tril_indices(res_num_rows, res_num_rows, offset=-1)\n    inf_triang_vals = corr_mat[tril_indices[0], tril_indices[1]]\n\n    return torch.abs(inf_triang_vals)\n</code></pre>"},{"location":"wgan_gp/pymfe_to_torch/#wgan_gp.pymfe_to_torch.MFEToTorch.ft_cov_torch","title":"<code>ft_cov_torch(N)</code>","text":"<p>Calculates the absolute values of the lower triangular elements of the covariance matrix. This focuses on the relationships between variables, extracting the lower triangle to reduce redundancy and focusing on key covariance values. The absolute value ensures that the magnitude of the covariance is considered, regardless of the direction of the relationship.</p> <p>Parameters:</p> Name Type Description Default <code>N</code> <code>Tensor</code> <p>Input tensor for covariance calculation.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the absolute values of the lower triangular elements of the covariance matrix.</p> Source code in <code>wgan_gp/pymfe_to_torch.py</code> <pre><code>def ft_cov_torch(\n    self,\n    N: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Calculates the absolute values of the lower triangular elements of the covariance matrix. This focuses on the relationships between variables, extracting the lower triangle to reduce redundancy and focusing on key covariance values. The absolute value ensures that the magnitude of the covariance is considered, regardless of the direction of the relationship.\n\n    Args:\n        N: Input tensor for covariance calculation.\n\n    Returns:\n        torch.Tensor: A tensor containing the absolute values of the lower triangular elements of the covariance matrix.\n    \"\"\"\n    cov_mat = self.cov(N, rowvar=False)\n\n    res_num_rows = cov_mat.shape[0]\n    tril_indices = torch.tril_indices(res_num_rows, res_num_rows, offset=-1)\n    inf_triang_vals = cov_mat[tril_indices[0], tril_indices[1]]\n\n    return torch.abs(inf_triang_vals)\n</code></pre>"},{"location":"wgan_gp/pymfe_to_torch/#wgan_gp.pymfe_to_torch.MFEToTorch.ft_eigenvals","title":"<code>ft_eigenvals(x)</code>","text":"<p>Computes the eigenvalues of the covariance matrix of the input tensor.</p> <p>This function is crucial for assessing the diversity and information content of the input data. By calculating the eigenvalues of the covariance matrix, we gain insights into the principal components and variance distribution within the data, which helps to ensure the generated synthetic data retains the key statistical characteristics of the original data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The eigenvalues of the covariance matrix.</p> Source code in <code>wgan_gp/pymfe_to_torch.py</code> <pre><code>def ft_eigenvals(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the eigenvalues of the covariance matrix of the input tensor.\n\n    This function is crucial for assessing the diversity and information\n    content of the input data. By calculating the eigenvalues of the\n    covariance matrix, we gain insights into the principal components\n    and variance distribution within the data, which helps to ensure\n    the generated synthetic data retains the key statistical\n    characteristics of the original data.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n\n    Returns:\n        torch.Tensor: The eigenvalues of the covariance matrix.\n    \"\"\"\n    # taking real part of first two eigenvals\n    centered = x - x.mean(dim=0, keepdim=True)\n    covs = self.cov(centered, rowvar=False)\n    return torch.linalg.eigvalsh(covs)\n</code></pre>"},{"location":"wgan_gp/pymfe_to_torch/#wgan_gp.pymfe_to_torch.MFEToTorch.ft_gravity_torch","title":"<code>ft_gravity_torch(N, y, norm_ord=2, classes=None, class_freqs=None, cls_inds=None)</code>  <code>staticmethod</code>","text":"<p>Computes the gravity between the majority and minority classes.</p> <p>This method calculates the distance between the mean feature vectors of the majority and minority classes. This distance serves as a measure of class separation in the feature space. By computing this \"gravity,\" the method quantifies the dissimilarity between the most and least frequent classes, providing insight into the dataset's class distribution and feature representation. This information can be valuable for assessing the quality and representativeness of generated synthetic data compared to real data.</p> <p>Parameters:</p> Name Type Description Default <code>N</code> <code>Tensor</code> <p>Feature tensor of shape (num_instances, num_features).</p> required <code>y</code> <code>Tensor</code> <p>Target tensor of shape (num_instances,).</p> required <code>norm_ord</code> <code>Union[int, float]</code> <p>Order of the norm to compute the distance (e.g., 2 for Euclidean). Defaults to 2.</p> <code>2</code> <code>classes</code> <code>Optional[Tensor]</code> <p>Optional tensor of unique class labels. If None, it's computed from <code>y</code>.</p> <code>None</code> <code>class_freqs</code> <code>Optional[Tensor]</code> <p>Optional tensor of class frequencies. If None, it's computed from <code>y</code>.</p> <code>None</code> <code>cls_inds</code> <code>Optional[Tensor]</code> <p>Optional list of indices for each class. If provided, it uses these indices to select instances.</p> <code>None</code> <p>Returns:</p> Type Description <p>torch.Tensor: The gravity value, representing the distance between the class centers.</p> Source code in <code>wgan_gp/pymfe_to_torch.py</code> <pre><code>@staticmethod\ndef ft_gravity_torch(\n    N: torch.Tensor,\n    y: torch.Tensor,\n    norm_ord: Union[int, float] = 2,\n    classes: Optional[torch.Tensor] = None,\n    class_freqs: Optional[torch.Tensor] = None,\n    cls_inds: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Computes the gravity between the majority and minority classes.\n\n    This method calculates the distance between the mean feature vectors of the\n    majority and minority classes. This distance serves as a measure of class\n    separation in the feature space. By computing this \"gravity,\" the method\n    quantifies the dissimilarity between the most and least frequent classes,\n    providing insight into the dataset's class distribution and feature\n    representation. This information can be valuable for assessing the quality\n    and representativeness of generated synthetic data compared to real data.\n\n    Args:\n        N: Feature tensor of shape (num_instances, num_features).\n        y: Target tensor of shape (num_instances,).\n        norm_ord: Order of the norm to compute the distance (e.g., 2 for Euclidean). Defaults to 2.\n        classes: Optional tensor of unique class labels. If None, it's computed from `y`.\n        class_freqs: Optional tensor of class frequencies. If None, it's computed from `y`.\n        cls_inds: Optional list of indices for each class. If provided, it uses these indices to select instances.\n\n    Returns:\n        torch.Tensor: The gravity value, representing the distance between the class centers.\n    \"\"\"\n    if classes is None or class_freqs is None:\n        classes, class_freqs = torch.unique(y, return_counts=True)\n\n    ind_cls_maj = torch.argmax(class_freqs)\n    class_maj = classes[ind_cls_maj]\n\n    remaining_classes = torch.cat(\n        (classes[:ind_cls_maj], classes[ind_cls_maj + 1 :])\n    )\n    remaining_freqs = torch.cat(\n        (class_freqs[:ind_cls_maj], class_freqs[ind_cls_maj + 1 :])\n    )\n\n    ind_cls_min = torch.argmin(remaining_freqs)\n\n    if cls_inds is not None:\n        insts_cls_maj = N[cls_inds[ind_cls_maj]]\n        if ind_cls_min &gt;= ind_cls_maj:\n            ind_cls_min += 1\n        insts_cls_min = N[cls_inds[ind_cls_min]]\n    else:\n        class_min = remaining_classes[ind_cls_min]\n        insts_cls_maj = N[y == class_maj]\n        insts_cls_min = N[y == class_min]\n\n    center_maj = insts_cls_maj.mean(dim=0)\n    center_min = insts_cls_min.mean(dim=0)\n    gravity = torch.norm(center_maj - center_min, p=norm_ord)\n\n    return gravity\n</code></pre>"},{"location":"wgan_gp/pymfe_to_torch/#wgan_gp.pymfe_to_torch.MFEToTorch.ft_iq_range","title":"<code>ft_iq_range(X)</code>  <code>staticmethod</code>","text":"<p>Calculates the interquartile range (IQR) of a tensor along the first dimension.</p> <p>The IQR is a measure of statistical dispersion, representing the difference between the 75th and 25th percentiles. This is useful for understanding the spread of the data, which helps to assess the utility of generated synthetic data by comparing its distribution to the real data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>The input tensor of shape [num_samples, num_features].</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The interquartile range of the input tensor, with shape [num_features]. This represents the spread of each feature across the samples.</p> Source code in <code>wgan_gp/pymfe_to_torch.py</code> <pre><code>@staticmethod\ndef ft_iq_range(X: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Calculates the interquartile range (IQR) of a tensor along the first dimension.\n\n    The IQR is a measure of statistical dispersion, representing the difference between the 75th and 25th percentiles. This is useful for understanding the spread of the data, which helps to assess the utility of generated synthetic data by comparing its distribution to the real data.\n\n    Args:\n        X: The input tensor of shape [num_samples, num_features].\n\n    Returns:\n        The interquartile range of the input tensor, with shape [num_features]. This represents the spread of each feature across the samples.\n    \"\"\"\n    q75, q25 = torch.quantile(X, 0.75, dim=0), torch.quantile(X, 0.25, dim=0)\n    iqr = q75 - q25  # shape: [num_features]\n    return iqr\n</code></pre>"},{"location":"wgan_gp/pymfe_to_torch/#wgan_gp.pymfe_to_torch.MFEToTorch.ft_kurtosis","title":"<code>ft_kurtosis(x)</code>  <code>staticmethod</code>","text":"<p>Calculates the kurtosis of a tensor.</p> <p>This function computes the kurtosis of the input tensor <code>x</code>, a statistical measure describing the shape of the data's distribution, specifically its tailedness. By calculating kurtosis, we can assess how well the generated data's distribution matches that of the real data, ensuring the synthetic data retains similar statistical properties. This is crucial for maintaining the utility of the generated data in downstream tasks.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The kurtosis of the input tensor.</p> Source code in <code>wgan_gp/pymfe_to_torch.py</code> <pre><code>@staticmethod\ndef ft_kurtosis(x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Calculates the kurtosis of a tensor.\n\n    This function computes the kurtosis of the input tensor `x`, a statistical measure\n    describing the shape of the data's distribution, specifically its tailedness.\n    By calculating kurtosis, we can assess how well the generated data's distribution\n    matches that of the real data, ensuring the synthetic data retains similar statistical\n    properties. This is crucial for maintaining the utility of the generated data in downstream tasks.\n\n    Args:\n        x (torch.Tensor): Input tensor.\n\n    Returns:\n        torch.Tensor: The kurtosis of the input tensor.\n    \"\"\"\n    mean = torch.mean(x)\n    diffs = x - mean\n    var = torch.mean(torch.pow(diffs, 2.0))\n    std = torch.pow(var, 0.5)\n    zscores = diffs / std\n    kurtoses = torch.mean(torch.pow(zscores, 4.0)) - 3.0\n    return kurtoses\n</code></pre>"},{"location":"wgan_gp/pymfe_to_torch/#wgan_gp.pymfe_to_torch.MFEToTorch.ft_mad","title":"<code>ft_mad(x, factor=1.4826)</code>  <code>staticmethod</code>","text":"<p>Compute the Median Absolute Deviation (MAD) of a tensor.</p> <p>The MAD is a robust measure of statistical dispersion, useful for understanding the spread of data in both real and synthetic datasets. It helps assess how well the generated data captures the variability present in the original data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <code>factor</code> <code>float</code> <p>A scaling factor to make the MAD an unbiased estimator of the standard deviation for normal data. Default is 1.4826, which applies when the data is normally distributed.</p> <code>1.4826</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The MAD of the input tensor.</p> Source code in <code>wgan_gp/pymfe_to_torch.py</code> <pre><code>@staticmethod\ndef ft_mad(x: torch.Tensor, factor: float = 1.4826) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the Median Absolute Deviation (MAD) of a tensor.\n\n    The MAD is a robust measure of statistical dispersion, useful for\n    understanding the spread of data in both real and synthetic datasets.\n    It helps assess how well the generated data captures the variability\n    present in the original data.\n\n    Args:\n        x: The input tensor.\n        factor: A scaling factor to make the MAD an unbiased estimator of the\n            standard deviation for normal data. Default is 1.4826, which\n            applies when the data is normally distributed.\n\n    Returns:\n        torch.Tensor: The MAD of the input tensor.\n    \"\"\"\n    m = x.median(dim=0, keepdim=True).values\n    ama = torch.abs(x - m)\n    mama = ama.median(dim=0).values\n    return mama / (1 / factor)\n</code></pre>"},{"location":"wgan_gp/pymfe_to_torch/#wgan_gp.pymfe_to_torch.MFEToTorch.ft_max","title":"<code>ft_max(N)</code>  <code>staticmethod</code>","text":"<p>Finds the maximum value in a tensor along dimension 0. This is used to identify the most prominent features across a dataset, which is crucial for maintaining data utility in generated synthetic data.</p> <p>Parameters:</p> Name Type Description Default <code>N</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the maximum values along dimension 0.</p> Source code in <code>wgan_gp/pymfe_to_torch.py</code> <pre><code>@staticmethod\ndef ft_max(N: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Finds the maximum value in a tensor along dimension 0. This is used to identify the most prominent features across a dataset, which is crucial for maintaining data utility in generated synthetic data.\n\n    Args:\n        N (torch.Tensor): The input tensor.\n\n    Returns:\n        torch.Tensor: A tensor containing the maximum values along dimension 0.\n    \"\"\"\n    return N.max(dim=0, keepdim=False).values\n</code></pre>"},{"location":"wgan_gp/pymfe_to_torch/#wgan_gp.pymfe_to_torch.MFEToTorch.ft_mean","title":"<code>ft_mean(N)</code>  <code>staticmethod</code>","text":"<p>Computes the mean of a tensor along the first dimension to aggregate information across samples. This is useful for summarizing the central tendency of features in the generated or real data.</p> <p>Parameters:</p> Name Type Description Default <code>N</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The mean of the input tensor along dimension 0.</p> Source code in <code>wgan_gp/pymfe_to_torch.py</code> <pre><code>@staticmethod\ndef ft_mean(N: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the mean of a tensor along the first dimension to aggregate information across samples. This is useful for summarizing the central tendency of features in the generated or real data.\n\n    Args:\n        N (torch.Tensor): The input tensor.\n\n    Returns:\n        torch.Tensor: The mean of the input tensor along dimension 0.\n    \"\"\"\n    return N.mean(dim=0)\n</code></pre>"},{"location":"wgan_gp/pymfe_to_torch/#wgan_gp.pymfe_to_torch.MFEToTorch.ft_median","title":"<code>ft_median(N)</code>  <code>staticmethod</code>","text":"<p>Calculates the median of a tensor along the first dimension. This is used to derive a representative central tendency of the data distribution, which is a crucial aspect of maintaining data utility in synthetic data generation.</p> <p>Parameters:</p> Name Type Description Default <code>N</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the median values along the first dimension.</p> Source code in <code>wgan_gp/pymfe_to_torch.py</code> <pre><code>@staticmethod\ndef ft_median(N: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Calculates the median of a tensor along the first dimension. This is used to derive a representative central tendency of the data distribution, which is a crucial aspect of maintaining data utility in synthetic data generation.\n\n    Args:\n        N: The input tensor.\n\n    Returns:\n        torch.Tensor: A tensor containing the median values along the first dimension.\n    \"\"\"\n    return N.median(dim=0).values\n</code></pre>"},{"location":"wgan_gp/pymfe_to_torch/#wgan_gp.pymfe_to_torch.MFEToTorch.ft_min","title":"<code>ft_min(N)</code>  <code>staticmethod</code>","text":"<p>Finds the minimum value of a tensor along dimension 0, which is useful for identifying the smallest values across different samples when comparing real and synthetic data distributions.</p> <p>Parameters:</p> Name Type Description Default <code>N</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the minimum values along dimension 0. This represents the minimum feature values across the dataset, aiding in the comparison of feature ranges between real and synthetic datasets.</p> Source code in <code>wgan_gp/pymfe_to_torch.py</code> <pre><code>@staticmethod\ndef ft_min(N: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Finds the minimum value of a tensor along dimension 0, which is useful for identifying the smallest values across different samples when comparing real and synthetic data distributions.\n\n    Args:\n        N (torch.Tensor): The input tensor.\n\n    Returns:\n        torch.Tensor: A tensor containing the minimum values along dimension 0. This represents the minimum feature values across the dataset, aiding in the comparison of feature ranges between real and synthetic datasets.\n    \"\"\"\n    return N.min(dim=0).values\n</code></pre>"},{"location":"wgan_gp/pymfe_to_torch/#wgan_gp.pymfe_to_torch.MFEToTorch.ft_range","title":"<code>ft_range(N)</code>  <code>staticmethod</code>","text":"<p>Calculates the range of values (max - min) along the first dimension (dimension 0) of the input tensor. This is useful for understanding the spread or variability of the data along that dimension, which helps assess how well the generated data captures the characteristics of the original data.</p> <p>Parameters:</p> Name Type Description Default <code>N</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the range (max - min) of values along dimension 0.</p> Source code in <code>wgan_gp/pymfe_to_torch.py</code> <pre><code>@staticmethod\ndef ft_range(N: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Calculates the range of values (max - min) along the first dimension (dimension 0) of the input tensor. This is useful for understanding the spread or variability of the data along that dimension, which helps assess how well the generated data captures the characteristics of the original data.\n\n    Args:\n        N: The input tensor.\n\n    Returns:\n        torch.Tensor: A tensor containing the range (max - min) of values along dimension 0.\n    \"\"\"\n    return N.max(dim=0).values - N.min(dim=0).values\n</code></pre>"},{"location":"wgan_gp/pymfe_to_torch/#wgan_gp.pymfe_to_torch.MFEToTorch.ft_skewness","title":"<code>ft_skewness(x)</code>  <code>staticmethod</code>","text":"<p>Computes the skewness of a tensor.</p> <p>This function calculates the skewness of the input tensor, a key statistical measure reflecting the asymmetry of the data distribution. Preserving this characteristic is crucial when generating synthetic data to maintain the real data's statistical properties.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The skewness of the input tensor.</p> Source code in <code>wgan_gp/pymfe_to_torch.py</code> <pre><code>@staticmethod\ndef ft_skewness(x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the skewness of a tensor.\n\n    This function calculates the skewness of the input tensor, a key statistical\n    measure reflecting the asymmetry of the data distribution. Preserving this characteristic\n    is crucial when generating synthetic data to maintain the real data's statistical properties.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n\n    Returns:\n        torch.Tensor: The skewness of the input tensor.\n    \"\"\"\n    mean = torch.mean(x)\n    diffs = x - mean\n    var = torch.mean(torch.pow(diffs, 2.0))\n    std = torch.pow(var, 0.5)\n    zscores = diffs / std\n    skews = torch.mean(torch.pow(zscores, 3.0))\n    return skews\n</code></pre>"},{"location":"wgan_gp/pymfe_to_torch/#wgan_gp.pymfe_to_torch.MFEToTorch.ft_sparsity","title":"<code>ft_sparsity(N)</code>","text":"<p>Calculates the feature sparsity of a given tensor.</p> <p>This method computes the sparsity of each feature in the input tensor <code>N</code>. Sparsity is defined as the ratio of the total number of instances to the number of unique values for each feature, normalized to the range [0, 1]. This metric helps to assess the diversity of feature values, which is crucial for generating synthetic data that accurately reflects the statistical properties of the original dataset. By quantifying feature sparsity, we can ensure that the generated data maintains a similar level of variability as the real data, thereby preserving its utility for downstream tasks.</p> <p>Parameters:</p> Name Type Description Default <code>N</code> <code>Tensor</code> <p>A tensor of shape (num_instances, num_features) representing the input data.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor of shape (num_features,) containing the sparsity</p> <code>Tensor</code> <p>score for each feature, normalized to the range [0, 1]. The tensor is</p> <code>Tensor</code> <p>moved to the device specified by <code>self.device</code>.</p> Source code in <code>wgan_gp/pymfe_to_torch.py</code> <pre><code>def ft_sparsity(self, N: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Calculates the feature sparsity of a given tensor.\n\n    This method computes the sparsity of each feature in the input tensor `N`.\n    Sparsity is defined as the ratio of the total number of instances to the\n    number of unique values for each feature, normalized to the range [0, 1].\n    This metric helps to assess the diversity of feature values, which is crucial\n    for generating synthetic data that accurately reflects the statistical\n    properties of the original dataset. By quantifying feature sparsity, we can\n    ensure that the generated data maintains a similar level of variability\n    as the real data, thereby preserving its utility for downstream tasks.\n\n    Args:\n        N (torch.Tensor): A tensor of shape (num_instances, num_features) representing the input data.\n\n    Returns:\n        torch.Tensor: A tensor of shape (num_features,) containing the sparsity\n        score for each feature, normalized to the range [0, 1]. The tensor is\n        moved to the device specified by `self.device`.\n    \"\"\"\n    ans = torch.tensor([attr.size(0) / torch.unique(attr).size(0) for attr in N.T])\n\n    num_inst = N.size(0)\n    norm_factor = 1.0 / (num_inst - 1.0)\n    result = (ans - 1.0) * norm_factor\n\n    return result.to(self.device)\n</code></pre>"},{"location":"wgan_gp/pymfe_to_torch/#wgan_gp.pymfe_to_torch.MFEToTorch.ft_std","title":"<code>ft_std(N)</code>  <code>staticmethod</code>","text":"<p>Calculates the standard deviation of a tensor along the first dimension (dimension 0). This is used to understand the spread or dispersion of the generated synthetic data across different samples, ensuring the generated data maintains a similar statistical distribution to the real data.</p> <p>Parameters:</p> Name Type Description Default <code>N</code> <code>Tensor</code> <p>The input tensor representing a batch of generated samples.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The standard deviation of the input tensor along dimension 0, representing the standard deviation for each feature across the generated samples.</p> Source code in <code>wgan_gp/pymfe_to_torch.py</code> <pre><code>@staticmethod\ndef ft_std(N):\n    \"\"\"\n    Calculates the standard deviation of a tensor along the first dimension (dimension 0). This is used to understand the spread or dispersion of the generated synthetic data across different samples, ensuring the generated data maintains a similar statistical distribution to the real data.\n\n    Args:\n        N (torch.Tensor): The input tensor representing a batch of generated samples.\n\n    Returns:\n        torch.Tensor: The standard deviation of the input tensor along dimension 0, representing the standard deviation for each feature across the generated samples.\n    \"\"\"\n    return torch.std(N, dim=0)\n</code></pre>"},{"location":"wgan_gp/pymfe_to_torch/#wgan_gp.pymfe_to_torch.MFEToTorch.ft_var","title":"<code>ft_var(N)</code>  <code>staticmethod</code>","text":"<p>Calculates the variance of a tensor along dimension 0. This is a crucial step in assessing the statistical similarity between real and synthetic datasets generated by the GAN, ensuring that the generated data captures the variability present in the original data.</p> <p>Parameters:</p> Name Type Description Default <code>N</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The variance of the input tensor along dimension 0.</p> Source code in <code>wgan_gp/pymfe_to_torch.py</code> <pre><code>@staticmethod\ndef ft_var(N):\n    \"\"\"\n    Calculates the variance of a tensor along dimension 0. This is a crucial step in assessing the statistical similarity between real and synthetic datasets generated by the GAN, ensuring that the generated data captures the variability present in the original data.\n\n    Args:\n        N (torch.Tensor): The input tensor.\n\n    Returns:\n        torch.Tensor: The variance of the input tensor along dimension 0.\n    \"\"\"\n    return torch.var(N, dim=0)\n</code></pre>"},{"location":"wgan_gp/pymfe_to_torch/#wgan_gp.pymfe_to_torch.MFEToTorch.get_mfs","title":"<code>get_mfs(X, y, subset=None)</code>","text":"<p>Computes a set of meta-features on the input data. These meta-features capture essential characteristics of the dataset, which is crucial for evaluating and ensuring the utility of synthetic data generated by GANs.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>The input data tensor.</p> required <code>y</code> <code>Tensor</code> <p>The target variable tensor. Required if 'gravity' is in the subset.</p> required <code>subset</code> <code>list of str</code> <p>A list of meta-feature names to compute. If None, defaults to ['mean', 'var'].</p> <code>None</code> <p>Returns:</p> Type Description <p>torch.Tensor: A tensor containing the computed meta-features, padded to the maximum shape among the computed features and stacked into a single tensor. This allows for consistent representation and comparison of different meta-features.</p> Source code in <code>wgan_gp/pymfe_to_torch.py</code> <pre><code>def get_mfs(self, X, y, subset=None):\n    \"\"\"\n    Computes a set of meta-features on the input data. These meta-features capture essential characteristics of the dataset, which is crucial for evaluating and ensuring the utility of synthetic data generated by GANs.\n\n    Args:\n        X (torch.Tensor): The input data tensor.\n        y (torch.Tensor, optional): The target variable tensor. Required if 'gravity' is in the subset.\n        subset (list of str, optional): A list of meta-feature names to compute. If None, defaults to ['mean', 'var'].\n\n    Returns:\n        torch.Tensor: A tensor containing the computed meta-features, padded to the maximum shape among the computed features and stacked into a single tensor. This allows for consistent representation and comparison of different meta-features.\n    \"\"\"\n    if subset is None:\n        subset = [\"mean\", \"var\"]\n\n    mfs = []\n    for name in subset:\n        if name not in self.feature_methods:\n            raise ValueError(f\"Unsupported meta-feature: '{name}'\")\n\n        if name == \"gravity\":\n            if y is None:\n                raise ValueError(\"Meta-feature 'gravity' requires `y`.\")\n            res = self.feature_methods[name](X, y)\n            res = torch.tile(res, (X.shape[-1],))  # match dimensionality\n        else:\n            res = self.feature_methods[name](X)\n\n        mfs.append(res)\n    shapes = [i.shape.numel() for i in mfs]\n    mfs = [self.pad_only(mf, max(shapes)) for mf in mfs]\n    return torch.stack(mfs)\n</code></pre>"},{"location":"wgan_gp/pymfe_to_torch/#wgan_gp.pymfe_to_torch.MFEToTorch.pad_only","title":"<code>pad_only(tensor, target_len)</code>","text":"<p>Pads a tensor with zeros to a specified length, ensuring consistent input sizes for subsequent processing steps. This is particularly useful when dealing with variable-length sequences that need to be batched or processed by models requiring fixed-size inputs.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor to be padded.</p> required <code>target_len</code> <code>int</code> <p>The desired length of the padded tensor.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The padded tensor, or the original tensor if its length is already greater than or equal to <code>target_len</code>.</p> Source code in <code>wgan_gp/pymfe_to_torch.py</code> <pre><code>def pad_only(self, tensor, target_len):\n    \"\"\"\n    Pads a tensor with zeros to a specified length, ensuring consistent input sizes for subsequent processing steps. This is particularly useful when dealing with variable-length sequences that need to be batched or processed by models requiring fixed-size inputs.\n\n    Args:\n        tensor (torch.Tensor): The input tensor to be padded.\n        target_len (int): The desired length of the padded tensor.\n\n    Returns:\n        torch.Tensor: The padded tensor, or the original tensor if its length is already greater than or equal to `target_len`.\n    \"\"\"\n    if tensor.shape[0] &lt; target_len:\n        padding = torch.zeros(target_len - tensor.shape[0]).to(self.device)\n        return torch.cat([tensor, padding])\n\n    return tensor\n</code></pre>"},{"location":"wgan_gp/pymfe_to_torch/#wgan_gp.pymfe_to_torch.MFEToTorch.test_me","title":"<code>test_me(subset=None)</code>","text":"<p>Compares meta-feature extraction using the <code>pymfe</code> package and the <code>MFEToTorch</code> class.</p> <p>This method fetches the California Housing dataset, extracts meta-features using both <code>pymfe</code> and the <code>MFEToTorch</code> class, and then compares the results. This comparison helps validate the correctness and consistency of the meta-feature extraction process implemented in the <code>MFEToTorch</code> class, ensuring that it aligns with established meta-feature extraction tools.</p> <p>Parameters:</p> Name Type Description Default <code>subset</code> <code>list</code> <p>A list of meta-features to extract. If None, defaults to [\"mean\", \"var\"].</p> <code>None</code> <p>Returns:</p> Type Description <p>pandas.DataFrame: A DataFrame containing the meta-features extracted by both <code>pymfe</code> and <code>MFEToTorch</code>, along with any discrepancies between the two.</p> Source code in <code>wgan_gp/pymfe_to_torch.py</code> <pre><code>def test_me(self, subset=None):\n    \"\"\"\n    Compares meta-feature extraction using the `pymfe` package and the `MFEToTorch` class.\n\n    This method fetches the California Housing dataset, extracts meta-features using both `pymfe` and the `MFEToTorch` class, and then compares the results. This comparison helps validate the correctness and consistency of the meta-feature extraction process implemented in the `MFEToTorch` class, ensuring that it aligns with established meta-feature extraction tools.\n\n    Args:\n        subset (list, optional): A list of meta-features to extract. If None, defaults to [\"mean\", \"var\"].\n\n    Returns:\n        pandas.DataFrame: A DataFrame containing the meta-features extracted by both `pymfe` and `MFEToTorch`, along with any discrepancies between the two.\n    \"\"\"\n    if subset is None:\n        subset = [\"mean\", \"var\"]\n\n    from sklearn.datasets import fetch_california_housing\n\n    bunch = fetch_california_housing(as_frame=True)\n    X, y = bunch.data, bunch.target\n    print(f\"Init data shape: {X.shape} + {y.shape}\")\n\n    mfe = MFE(groups=\"statistical\", summary=None)\n    mfe.fit(X.values, y.values)\n    ft = mfe.extract()\n\n    pymfe = pd.DataFrame(\n        map(lambda x: [x], ft[1]), index=ft[0], columns=[\"pymfe\"]\n    ).dropna()\n\n    X_tensor = torch.tensor(X.values)\n    y_tensor = torch.tensor(y)\n\n    mfs = self.get_mfs(X_tensor, y_tensor, subset).numpy()\n    mfs_df = pd.DataFrame({\"torch_mfs\": list(mfs)})\n\n    mfs_df.index = subset\n    # mfs_df = mfs_df.reindex(self.mfs_available)\n\n    res = pymfe.merge(mfs_df, left_index=True, right_index=True, how=\"outer\")\n\n    def round_element(val, decimals=2):\n        if isinstance(val, list):\n            return [round(x, decimals) for x in val]\n        elif isinstance(val, np.ndarray):\n            return np.round(val, decimals)\n        return round(val, decimals)\n\n    res = res.map(lambda x: round_element(x, 5)).dropna()\n\n    print(res)\n</code></pre>"},{"location":"wgan_gp/training/","title":"Training Module","text":"<p>This module contains the core training classes for WGAN-GP with Meta-Feature Statistics (MFS) preservation.</p>"},{"location":"wgan_gp/training/#classes","title":"Classes","text":""},{"location":"wgan_gp/training/#trainer","title":"Trainer","text":"<p>Base trainer class for vanilla WGAN-GP implementation with gradient penalty.</p>"},{"location":"wgan_gp/training/#trainermodified","title":"TrainerModified","text":"<p>Enhanced trainer class that incorporates Meta-Feature Statistics preservation during training.</p>"},{"location":"wgan_gp/training/#key-features","title":"Key Features","text":"<ul> <li>Wasserstein Distance with Gradient Penalty: Stable GAN training using WGAN-GP formulation</li> <li>Meta-Feature Statistics Preservation: Maintains statistical properties of original data</li> <li>Flexible Loss Weighting: Configurable balance between adversarial and MFS losses</li> <li>Comprehensive Monitoring: Gradient flow visualization and training metrics tracking</li> <li>Experiment Tracking: Built-in Aim integration for training progress monitoring</li> </ul>"},{"location":"wgan_gp/training/#wgan_gp.training","title":"<code>wgan_gp.training</code>","text":""},{"location":"wgan_gp/training/#wgan_gp.training.Trainer","title":"<code>Trainer</code>","text":"<p>A base class for training generative adversarial networks (GANs).</p> <p>This class provides a basic structure for training GANs, including methods for training the discriminator and generator, calculating gradient penalties, and generating samples.</p> Source code in <code>wgan_gp/training.py</code> <pre><code>class Trainer:\n    \"\"\"\n    A base class for training generative adversarial networks (GANs).\n\n    This class provides a basic structure for training GANs, including methods for training\n    the discriminator and generator, calculating gradient penalties, and generating samples.\n    \"\"\"\n\n    def __init__(\n        self,\n        generator,\n        discriminator,\n        gen_optimizer,\n        dis_optimizer,\n        batch_size,\n        aim_track,\n        gen_model_name,\n        disable_tqdm=False,\n        gp_weight=10,\n        critic_iterations=5,\n        device=torch.device(\"cpu\"),\n    ):\n        \"\"\"\n        Initializes the WGAN-GP trainer, setting up the necessary components for adversarial training\n        to generate synthetic data.\n        This method prepares the generator and discriminator networks, configures their respective\n        optimizers, and establishes the training loop parameters. It's crucial for ensuring that\n        the GAN training process is correctly initialized, allowing the generator to learn how to\n        create realistic synthetic samples while the discriminator learns to distinguish between\n        real and generated data.\n\n        Args:\n            generator: The generator network.\n            discriminator: The discriminator network.\n            gen_optimizer: The optimizer for the generator.\n            dis_optimizer: The optimizer for the discriminator.\n            batch_size: The batch size for training.\n            aim_track: A dictionary for tracking training progress with Aim.\n            gen_model_name: The name of the generator model.\n            disable_tqdm: Whether to disable tqdm progress bar. Defaults to False.\n            gp_weight: The weight of the gradient penalty. Defaults to 10.\n            critic_iterations: The number of discriminator iterations per generator iteration.\n                Defaults to 5.\n            device: The device to use for training (e.g., 'cuda' or 'cpu').\n                Defaults to torch.device('cpu').\n        \"\"\"\n        self.G = generator\n        self.G_opt = gen_optimizer\n        self.D = discriminator\n        self.D_opt = dis_optimizer\n        self.losses = {\"G\": [], \"D\": [], \"GP\": [], \"gradient_norm\": []}\n        self.num_steps = 0\n        self.device = device\n        self.gp_weight = gp_weight\n        self.critic_iterations = critic_iterations\n\n        self.batch_size = batch_size\n        self.num_batches_per_epoch = 0\n\n        self.aim_track = aim_track\n        self.G.to(self.device)\n        self.D.to(self.device)\n\n        self.disable = disable_tqdm\n        self.aim_track[\"hparams\"] |= {\"gen_model\": gen_model_name}\n\n    @staticmethod\n    def total_grad_norm(model):\n        \"\"\"\n        Computes the total gradient norm of a model's parameters.\n\n        Calculates the L2 norm of the gradients across all parameters in the model.\n        This is useful for monitoring training and detecting potential issues like\n        exploding gradients, ensuring stable training during synthetic data generation.\n        By monitoring the gradient norm, we can ensure the generator and discriminator\n        are learning effectively and prevent instability, which is crucial for producing\n        high-quality synthetic data that preserves the utility of the original dataset.\n\n        Args:\n            model: The model whose gradients are to be evaluated.\n\n        Returns:\n            float: The total gradient norm.\n        \"\"\"\n        total_norm = 0\n        for p in model.parameters():\n            if p.grad is not None:\n                param_norm = p.grad.data.norm(2)\n                total_norm += param_norm.item() ** 2\n        return total_norm**0.5\n\n    def _critic_train_iteration(self, data):\n        \"\"\"\n        Trains the critic (discriminator) for one iteration to distinguish between real and\n        generated data.\n\n        The critic's objective is to maximize the difference between its output for real and\n        generated samples, along with a gradient penalty to enforce the Lipschitz constraint.\n        This step is crucial for improving the generator's ability to create realistic\n        synthetic data by providing a strong adversary.\n\n        Args:\n            data (torch.Tensor): A batch of real data samples.\n\n        Returns:\n            None. The critic's loss is tracked in `self.D_loss`.\n        \"\"\"\n        self.D_opt.zero_grad()\n        self.D.train()  # just to be explicit\n\n        # Move real data to device\n        data = data.to(\n            self.device\n        )  # assume self.device is torch.device('cuda' or 'cpu')\n\n        batch_size = data.size(0)\n\n        # Generate fake data\n        with torch.no_grad():  # generator isn't trained here, so we can disable grad\n            generated_data = self.sample_generator(batch_size)\n            generated_data = generated_data.to(self.device)\n\n        # Detach to be safe (in case generator outputs are connected to autograd graph)\n        generated_data = generated_data.detach()\n\n        # Discriminator outputs\n        d_real = self.D(data)\n        d_generated = self.D(generated_data)\n\n        # Gradient penalty\n        gradient_penalty = self._gradient_penalty(data, generated_data)\n\n        # Compute WGAN-GP loss\n        d_loss = d_generated.mean() - d_real.mean() + gradient_penalty\n\n        # Backprop\n        d_loss.backward()\n        self.D_opt.step()\n\n        # Track loss\n        self.D_loss = d_loss\n\n    def _generator_train_iteration(self, data):\n        \"\"\"\n        Performs a single training iteration for the generator network.\n\n        This involves sampling synthetic data, evaluating the generator's performance based\n        on the discriminator's output, and updating the generator's weights to improve the\n        quality of the generated samples.\n\n        Args:\n            data (torch.Tensor): A batch of real data (not used in this function, but present\n                for consistency with discriminator training).\n\n        Returns:\n            None. The generator loss is stored in `self.G_loss`.\n\n        Why:\n            The generator is trained to produce synthetic data that can fool the discriminator.\n            This function updates the generator's parameters to minimize the discriminator's\n            ability to distinguish between real and generated data, thereby improving the\n            realism and utility of the synthetic data.\n        \"\"\"\n        self.G_opt.zero_grad()\n\n        # Get generated data\n        batch_size = data.size(0)\n        generated_data = self.sample_generator(batch_size)\n\n        # Calculate loss and optimize\n        d_generated = self.D(generated_data)\n        g_loss = -d_generated.mean()\n        g_loss.backward()\n        self.G_opt.step()\n\n        # Record loss\n        self.G_loss = g_loss\n\n    def _gradient_penalty(self, real_data, generated_data):\n        \"\"\"\n        Computes the gradient penalty for WGAN-GP.\n\n        This method calculates the gradient penalty, a regularization term used in Wasserstein\n        GANs with gradient penalty (WGAN-GP). By penalizing the norm of discriminator gradients\n        with respect to its input, we encourage the discriminator to have a smoother landscape.\n        This enforces the Lipschitz constraint, which is crucial for the Wasserstein distance\n        to be a valid metric and for stable GAN training, ultimately improving the utility of\n        generated samples.\n\n        Args:\n            real_data (torch.Tensor): Real data samples.\n            generated_data (torch.Tensor): Generated data samples.\n\n        Returns:\n            torch.Tensor: The gradient penalty.\n        \"\"\"\n        batch_size = real_data.size(0)\n\n        # Sample interpolation factor\n        alpha = torch.rand(batch_size, 1).to(self.device)\n        alpha = alpha.expand_as(real_data)\n\n        # Interpolate between real and fake data\n        interpolated = alpha * real_data + (1 - alpha) * generated_data\n        interpolated.requires_grad_(True)\n\n        # Compute critic output on interpolated data\n        prob_interpolated = self.D(interpolated)\n\n        # Compute gradients\n        grad_outputs = torch.ones_like(prob_interpolated)\n        gradients = torch_grad(\n            outputs=prob_interpolated,\n            inputs=interpolated,\n            grad_outputs=grad_outputs,\n            create_graph=True,\n            retain_graph=True,\n            only_inputs=True,\n        )[0]\n\n        gradients = gradients.view(batch_size, -1)\n        gradients_norm = gradients.norm(2, dim=1)\n\n        self.GP_grad_norm = gradients_norm.mean().item()\n\n        # Compute penalty\n        gp = self.gp_weight * ((gradients_norm - 1) ** 2).mean()\n        return gp\n\n    def _train_epoch(self, data_loader):\n        \"\"\"\n        Trains the GAN for one epoch using the provided data loader, alternating between\n        critic and generator training steps.\n\n        The method iterates through the data loader, performing multiple critic updates for\n        each generator update, as determined by `self.critic_iterations`. This ensures the\n        critic is well-trained to differentiate between real and generated samples, which\n        is crucial for the generator to learn to produce realistic synthetic data.\n\n        Args:\n            data_loader: The data loader providing real data samples for training.\n\n        Returns:\n            None. The method updates the generator and critic networks in place to improve\n            the quality and utility of generated samples.\n        \"\"\"\n        data_iter = iter(data_loader)\n\n        for _ in range(self.num_batches_per_epoch):\n            # --- Critic updates ---\n            for _ in range(self.critic_iterations):\n                try:\n                    data = next(data_iter)\n                except StopIteration:\n                    data_iter = iter(data_loader)\n                    data = next(data_iter)\n                data = data.to(self.device)  # Ensure data is on the correct device\n                self._critic_train_iteration(data)\n                self.num_steps += 1\n\n            # --- Generator update ---\n            try:\n                data = next(data_iter)\n            except StopIteration:\n                data_iter = iter(data_loader)\n                data = next(data_iter)\n            data = data.to(self.device)  # Ensure data is on the correct device\n            self._generator_train_iteration(data)\n\n    def train(self, data_loader, epochs, plot_freq):\n        \"\"\"\n        Trains the GAN model to generate synthetic data that mimics the distribution of the real data.\n\n        The training process involves iteratively updating the generator and discriminator\n        networks to improve the quality and realism of the generated samples. The progress\n        is monitored and visualized through loss tracking and sample plotting.\n\n        Args:\n            data_loader: The data loader providing batches of real data for training.\n            epochs: The number of training epochs to perform.\n            plot_freq: The frequency (in epochs) at which to generate and plot samples to\n                visualize training progress.\n\n        Returns:\n            None\n        \"\"\"\n        pca = False\n        pbar = tqdm.tqdm(range(epochs), total=epochs, disable=self.disable)\n        self.loss_values = pd.DataFrame()\n        self.num_batches_per_epoch = len(data_loader)\n\n        for epoch in pbar:\n            self._train_epoch(data_loader)\n            pbar.set_description(f\"Epoch {epoch}\")\n\n            fig = plt.figure()\n            real_data_sample = next(iter(data_loader))\n\n            samples = self.sample_generator(self.batch_size).cpu().detach().numpy()\n            if samples.shape[1] &gt; 2:\n                pca = True\n                pca = PCA(n_components=2)\n                samples = pca.fit_transform(samples[:, :-1])\n                real_data_sample = pca.fit_transform(real_data_sample[:, :-1])\n\n            plt.scatter(samples[:, 0], samples[:, 1], label=\"Synthetic\", alpha=0.3)\n            plt.scatter(\n                real_data_sample[:, 0],\n                real_data_sample[:, 1],\n                label=\"Real data\",\n                alpha=0.3,\n            )\n\n            if pca:\n                plt.title(f\"Explained var: {sum(pca.explained_variance_ratio_)}\")\n\n            plt.legend()\n            plt.close(fig)\n\n            aim_fig = Image(fig)\n            if epoch % plot_freq == 0:\n                self.aim_track.track(aim_fig, epoch=epoch, name=\"progress\")\n\n            if self.aim_track:\n                self.aim_track.track(self.G_loss.item(), name=\"loss G\", epoch=epoch)\n                self.aim_track.track(self.D_loss.item(), name=\"loss D\", epoch=epoch)\n                self.aim_track.track(\n                    self.total_grad_norm(self.G), name=\"total_norm_G\", epoch=epoch\n                )\n                self.aim_track.track(\n                    self.total_grad_norm(self.D), name=\"total_norm_D\", epoch=epoch\n                )\n                self.aim_track.track(\n                    self.GP_grad_norm, name=\"GP_grad_norm\", epoch=epoch\n                )\n\n    def sample_generator(self, num_samples):\n        \"\"\"\n        Generates synthetic data samples using the generator network to augment the original dataset.\n\n        The generated samples aim to resemble the real data distribution, enhancing the dataset's\n        utility for downstream tasks.\n\n        Args:\n            num_samples: The number of synthetic samples to generate.\n\n        Returns:\n            The generated synthetic data samples.\n        \"\"\"\n        latent_samples = self.G.sample_latent(num_samples).to(self.device)\n        generated_data = self.G(latent_samples)\n        return generated_data\n</code></pre>"},{"location":"wgan_gp/training/#wgan_gp.training.Trainer.__init__","title":"<code>__init__(generator, discriminator, gen_optimizer, dis_optimizer, batch_size, aim_track, gen_model_name, disable_tqdm=False, gp_weight=10, critic_iterations=5, device=torch.device('cpu'))</code>","text":"<p>Initializes the WGAN-GP trainer, setting up the necessary components for adversarial training to generate synthetic data. This method prepares the generator and discriminator networks, configures their respective optimizers, and establishes the training loop parameters. It's crucial for ensuring that the GAN training process is correctly initialized, allowing the generator to learn how to create realistic synthetic samples while the discriminator learns to distinguish between real and generated data.</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <p>The generator network.</p> required <code>discriminator</code> <p>The discriminator network.</p> required <code>gen_optimizer</code> <p>The optimizer for the generator.</p> required <code>dis_optimizer</code> <p>The optimizer for the discriminator.</p> required <code>batch_size</code> <p>The batch size for training.</p> required <code>aim_track</code> <p>A dictionary for tracking training progress with Aim.</p> required <code>gen_model_name</code> <p>The name of the generator model.</p> required <code>disable_tqdm</code> <p>Whether to disable tqdm progress bar. Defaults to False.</p> <code>False</code> <code>gp_weight</code> <p>The weight of the gradient penalty. Defaults to 10.</p> <code>10</code> <code>critic_iterations</code> <p>The number of discriminator iterations per generator iteration. Defaults to 5.</p> <code>5</code> <code>device</code> <p>The device to use for training (e.g., 'cuda' or 'cpu'). Defaults to torch.device('cpu').</p> <code>device('cpu')</code> Source code in <code>wgan_gp/training.py</code> <pre><code>def __init__(\n    self,\n    generator,\n    discriminator,\n    gen_optimizer,\n    dis_optimizer,\n    batch_size,\n    aim_track,\n    gen_model_name,\n    disable_tqdm=False,\n    gp_weight=10,\n    critic_iterations=5,\n    device=torch.device(\"cpu\"),\n):\n    \"\"\"\n    Initializes the WGAN-GP trainer, setting up the necessary components for adversarial training\n    to generate synthetic data.\n    This method prepares the generator and discriminator networks, configures their respective\n    optimizers, and establishes the training loop parameters. It's crucial for ensuring that\n    the GAN training process is correctly initialized, allowing the generator to learn how to\n    create realistic synthetic samples while the discriminator learns to distinguish between\n    real and generated data.\n\n    Args:\n        generator: The generator network.\n        discriminator: The discriminator network.\n        gen_optimizer: The optimizer for the generator.\n        dis_optimizer: The optimizer for the discriminator.\n        batch_size: The batch size for training.\n        aim_track: A dictionary for tracking training progress with Aim.\n        gen_model_name: The name of the generator model.\n        disable_tqdm: Whether to disable tqdm progress bar. Defaults to False.\n        gp_weight: The weight of the gradient penalty. Defaults to 10.\n        critic_iterations: The number of discriminator iterations per generator iteration.\n            Defaults to 5.\n        device: The device to use for training (e.g., 'cuda' or 'cpu').\n            Defaults to torch.device('cpu').\n    \"\"\"\n    self.G = generator\n    self.G_opt = gen_optimizer\n    self.D = discriminator\n    self.D_opt = dis_optimizer\n    self.losses = {\"G\": [], \"D\": [], \"GP\": [], \"gradient_norm\": []}\n    self.num_steps = 0\n    self.device = device\n    self.gp_weight = gp_weight\n    self.critic_iterations = critic_iterations\n\n    self.batch_size = batch_size\n    self.num_batches_per_epoch = 0\n\n    self.aim_track = aim_track\n    self.G.to(self.device)\n    self.D.to(self.device)\n\n    self.disable = disable_tqdm\n    self.aim_track[\"hparams\"] |= {\"gen_model\": gen_model_name}\n</code></pre>"},{"location":"wgan_gp/training/#wgan_gp.training.Trainer.sample_generator","title":"<code>sample_generator(num_samples)</code>","text":"<p>Generates synthetic data samples using the generator network to augment the original dataset.</p> <p>The generated samples aim to resemble the real data distribution, enhancing the dataset's utility for downstream tasks.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <p>The number of synthetic samples to generate.</p> required <p>Returns:</p> Type Description <p>The generated synthetic data samples.</p> Source code in <code>wgan_gp/training.py</code> <pre><code>def sample_generator(self, num_samples):\n    \"\"\"\n    Generates synthetic data samples using the generator network to augment the original dataset.\n\n    The generated samples aim to resemble the real data distribution, enhancing the dataset's\n    utility for downstream tasks.\n\n    Args:\n        num_samples: The number of synthetic samples to generate.\n\n    Returns:\n        The generated synthetic data samples.\n    \"\"\"\n    latent_samples = self.G.sample_latent(num_samples).to(self.device)\n    generated_data = self.G(latent_samples)\n    return generated_data\n</code></pre>"},{"location":"wgan_gp/training/#wgan_gp.training.Trainer.total_grad_norm","title":"<code>total_grad_norm(model)</code>  <code>staticmethod</code>","text":"<p>Computes the total gradient norm of a model's parameters.</p> <p>Calculates the L2 norm of the gradients across all parameters in the model. This is useful for monitoring training and detecting potential issues like exploding gradients, ensuring stable training during synthetic data generation. By monitoring the gradient norm, we can ensure the generator and discriminator are learning effectively and prevent instability, which is crucial for producing high-quality synthetic data that preserves the utility of the original dataset.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The model whose gradients are to be evaluated.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The total gradient norm.</p> Source code in <code>wgan_gp/training.py</code> <pre><code>@staticmethod\ndef total_grad_norm(model):\n    \"\"\"\n    Computes the total gradient norm of a model's parameters.\n\n    Calculates the L2 norm of the gradients across all parameters in the model.\n    This is useful for monitoring training and detecting potential issues like\n    exploding gradients, ensuring stable training during synthetic data generation.\n    By monitoring the gradient norm, we can ensure the generator and discriminator\n    are learning effectively and prevent instability, which is crucial for producing\n    high-quality synthetic data that preserves the utility of the original dataset.\n\n    Args:\n        model: The model whose gradients are to be evaluated.\n\n    Returns:\n        float: The total gradient norm.\n    \"\"\"\n    total_norm = 0\n    for p in model.parameters():\n        if p.grad is not None:\n            param_norm = p.grad.data.norm(2)\n            total_norm += param_norm.item() ** 2\n    return total_norm**0.5\n</code></pre>"},{"location":"wgan_gp/training/#wgan_gp.training.Trainer.train","title":"<code>train(data_loader, epochs, plot_freq)</code>","text":"<p>Trains the GAN model to generate synthetic data that mimics the distribution of the real data.</p> <p>The training process involves iteratively updating the generator and discriminator networks to improve the quality and realism of the generated samples. The progress is monitored and visualized through loss tracking and sample plotting.</p> <p>Parameters:</p> Name Type Description Default <code>data_loader</code> <p>The data loader providing batches of real data for training.</p> required <code>epochs</code> <p>The number of training epochs to perform.</p> required <code>plot_freq</code> <p>The frequency (in epochs) at which to generate and plot samples to visualize training progress.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>wgan_gp/training.py</code> <pre><code>def train(self, data_loader, epochs, plot_freq):\n    \"\"\"\n    Trains the GAN model to generate synthetic data that mimics the distribution of the real data.\n\n    The training process involves iteratively updating the generator and discriminator\n    networks to improve the quality and realism of the generated samples. The progress\n    is monitored and visualized through loss tracking and sample plotting.\n\n    Args:\n        data_loader: The data loader providing batches of real data for training.\n        epochs: The number of training epochs to perform.\n        plot_freq: The frequency (in epochs) at which to generate and plot samples to\n            visualize training progress.\n\n    Returns:\n        None\n    \"\"\"\n    pca = False\n    pbar = tqdm.tqdm(range(epochs), total=epochs, disable=self.disable)\n    self.loss_values = pd.DataFrame()\n    self.num_batches_per_epoch = len(data_loader)\n\n    for epoch in pbar:\n        self._train_epoch(data_loader)\n        pbar.set_description(f\"Epoch {epoch}\")\n\n        fig = plt.figure()\n        real_data_sample = next(iter(data_loader))\n\n        samples = self.sample_generator(self.batch_size).cpu().detach().numpy()\n        if samples.shape[1] &gt; 2:\n            pca = True\n            pca = PCA(n_components=2)\n            samples = pca.fit_transform(samples[:, :-1])\n            real_data_sample = pca.fit_transform(real_data_sample[:, :-1])\n\n        plt.scatter(samples[:, 0], samples[:, 1], label=\"Synthetic\", alpha=0.3)\n        plt.scatter(\n            real_data_sample[:, 0],\n            real_data_sample[:, 1],\n            label=\"Real data\",\n            alpha=0.3,\n        )\n\n        if pca:\n            plt.title(f\"Explained var: {sum(pca.explained_variance_ratio_)}\")\n\n        plt.legend()\n        plt.close(fig)\n\n        aim_fig = Image(fig)\n        if epoch % plot_freq == 0:\n            self.aim_track.track(aim_fig, epoch=epoch, name=\"progress\")\n\n        if self.aim_track:\n            self.aim_track.track(self.G_loss.item(), name=\"loss G\", epoch=epoch)\n            self.aim_track.track(self.D_loss.item(), name=\"loss D\", epoch=epoch)\n            self.aim_track.track(\n                self.total_grad_norm(self.G), name=\"total_norm_G\", epoch=epoch\n            )\n            self.aim_track.track(\n                self.total_grad_norm(self.D), name=\"total_norm_D\", epoch=epoch\n            )\n            self.aim_track.track(\n                self.GP_grad_norm, name=\"GP_grad_norm\", epoch=epoch\n            )\n</code></pre>"},{"location":"wgan_gp/training/#wgan_gp.training.TrainerModified","title":"<code>TrainerModified</code>","text":"<p>               Bases: <code>Trainer</code></p> <p>A modified trainer class for training GANs with Meta-Feature Statistics (MFS) preservation.</p> <p>This class extends the base trainer to incorporate Meta-Feature Statistics (MFS) into the training process, allowing for targeted preservation of statistical properties and enhanced synthetic data quality.</p> Source code in <code>wgan_gp/training.py</code> <pre><code>class TrainerModified(Trainer):\n    \"\"\"\n    A modified trainer class for training GANs with Meta-Feature Statistics (MFS) preservation.\n\n    This class extends the base trainer to incorporate Meta-Feature Statistics (MFS)\n    into the training process, allowing for targeted preservation of statistical properties\n    and enhanced synthetic data quality.\n    \"\"\"\n\n    def __init__(self, mfs_lambda, subset_mfs, target_mfs, sample_number, **kwargs):\n        \"\"\"\n        Initializes the TrainerModified class with Meta-Feature Statistics preservation.\n\n        This class configures the training process for the GAN, focusing on preserving\n        meta-feature statistics to enhance the utility of generated synthetic data. It sets up\n        the parameters that guide the MFS preservation process, ensuring the generated data\n        retains key statistical characteristics of the real data.\n\n        Args:\n            mfs_lambda (float or list): Lambda value(s) for MFS loss weighting, controlling\n                the strength of the meta-feature preservation regularization.\n            subset_mfs (list): Subset of meta-features to preserve, defining which statistical\n                properties to focus on during training.\n            target_mfs (dict): Target MFS distributions, specifying the desired meta-feature\n                distributions in the generated data. Defaults to {\"other_mfs\": 0} if not provided.\n            sample_number (int): Number of variates to use during MFS calculation, influencing\n                the stability and accuracy of meta-feature estimation.\n            **kwargs: Additional keyword arguments passed to the parent Trainer class.\n\n        The method initializes the training process by setting up the meta-feature statistics (MFS)\n        parameters. This setup is crucial for guiding the GAN to generate synthetic data that not\n        only resembles the real data visually but also maintains its statistical utility. The\n        target_mfs parameter allows specifying the desired distribution of meta-features in the\n        generated data, ensuring that the synthetic data preserves important statistical properties\n        for downstream tasks.\n        \"\"\"\n        super(TrainerModified, self).__init__(**kwargs)\n        self.mfs_lambda = mfs_lambda\n        self.subset_mfs = subset_mfs\n\n        if not target_mfs:\n            target_mfs = {\"other_mfs\": 0}\n\n        self.target_mfs = target_mfs\n\n        if \"other_mfs\" in target_mfs.keys():\n            if isinstance(target_mfs[\"other_mfs\"], torch.Tensor):\n                self.target_mfs[\"other_mfs\"] = target_mfs[\"other_mfs\"].to(self.device)\n\n        self.mfs_manager = MFEToTorch()\n        self.wasserstein_dist_func = WassersteinDistance(q=2)\n        self.sample_number = sample_number\n\n    @staticmethod\n    def sample_from_tensor(tensor, n_samples):\n        \"\"\"\n        Samples a subset of data points from a given tensor.\n\n        This is useful for creating smaller, representative datasets for tasks such as\n        evaluating model performance on a subset of the data or for visualization purposes.\n\n        Args:\n            tensor (torch.Tensor): The input tensor from which to sample.\n            n_samples (int): The number of data points to sample from the tensor.\n\n        Returns:\n            torch.Tensor: A new tensor containing the sampled data points.\n                The sampled data maintains the original data's structure while reducing its size,\n                which is important for efficient analysis and evaluation.\n        \"\"\"\n        indices = torch.randperm(tensor.size(0))\n        indices_trunc = indices[:n_samples]\n        sampled_tensor = tensor[indices_trunc]\n        return sampled_tensor\n\n    def calculate_mfs_torch(\n        self, X: torch.Tensor, y: torch.Tensor = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Calculates the meta-feature statistics (MFS) to quantify statistical properties for\n        preserving data utility in GAN-generated synthetic data.\n\n        This method leverages the MFS manager to assess various statistical properties of the\n        input tensor X, optionally conditioned on a target tensor y. This helps in understanding\n        which statistical characteristics are most important for preserving data utility in\n        synthetic samples.\n\n        Args:\n            X (torch.Tensor): The input tensor representing the synthetic data features.\n            y (torch.Tensor, optional): The target tensor representing the corresponding\n                target variable. Defaults to None.\n\n        Returns:\n            torch.Tensor: The calculated MFS values, moved to the specified device.\n                These values represent various statistical properties of the data (correlation,\n                covariance, eigenvalues, etc.), which are used to guide the generator's\n                learning process and ensure statistical fidelity.\n        \"\"\"\n        return self.mfs_manager.get_mfs(X, y, subset=self.subset_mfs).to(self.device)\n\n    @staticmethod\n    def total_grad_norm(model):\n        \"\"\"\n        Computes the total gradient norm of a model's parameters.\n\n        Calculates the L2 norm of the gradients across all parameters in the model.\n        This is useful for monitoring training and detecting potential issues like\n        exploding gradients, ensuring stable training during synthetic data generation.\n        By monitoring the gradient norm, we can ensure the generator and discriminator\n        are learning effectively and preventing mode collapse, which is crucial for\n        producing high-quality synthetic data.\n\n        Args:\n            model: The model whose gradients are to be analyzed.\n\n        Returns:\n            float: The total gradient norm.\n        \"\"\"\n        total_norm = 0\n        for p in model.parameters():\n            if p.grad is not None:\n                param_norm = p.grad.data.norm(2)\n                total_norm += param_norm.item() ** 2\n        return total_norm**0.5\n\n    def compute_loss_on_variates_wasserstein(self, fake_distribution):\n        \"\"\"\n        Computes the Wasserstein loss to align generated and real data distributions.\n\n        This method calculates the Wasserstein distance between the target meta-feature\n        statistics (MFS) and the MFS generated from the fake data distribution. It first\n        calculates the MFS for each variate in the fake distribution, reshapes them, and\n        then computes the Wasserstein distance using the specified distance function. This\n        loss encourages the generator to produce data with similar statistical properties\n        to the real data, enhancing the utility of the synthetic data for downstream tasks.\n\n        Args:\n            fake_distribution: A list of tensors representing the generated data distribution.\n                Each tensor represents a variate.\n\n        Returns:\n            torch.Tensor: The Wasserstein distance between the target MFS and the generated MFS.\n        \"\"\"\n        fake_mfs = [self.calculate_mfs_torch(X) for X in fake_distribution]\n        fake_mfs = self.reshape_mfs_from_variates(fake_mfs)\n\n        # mfs_to_track = fake_mfs.clone()\n        # self.mfs_to_track = mfs_to_track.mean(dim=1).cpu().detach().numpy().round(5).tolist()\n        return self.wasserstein_dist_func(self.target_mfs[\"other_mfs\"], fake_mfs)\n\n    @staticmethod\n    def reshape_mfs_from_variates(mfs_from_variates: list):\n        \"\"\"\n        Reshapes a list of meta-feature statistics from variates into a tensor for comparison.\n\n        The input list `mfs_from_variates` contains MFS values, which are stacked\n        and then transposed to create the reshaped tensor. This reshaping\n        facilitates the calculation of metrics and topological analysis\n        needed to evaluate the quality and utility of the generated synthetic data.\n\n        Args:\n            mfs_from_variates: A list of meta-feature statistics from variates.\n\n        Returns:\n            torch.Tensor: A reshaped tensor where the first dimension corresponds\n                to the variates and the second dimension corresponds to the MFS values.\n                This format is required for subsequent analysis and comparison\n                of real and synthetic data characteristics.\n        \"\"\"\n        stacked = torch.stack(mfs_from_variates)\n        reshaped = stacked.transpose(0, 1)\n        return reshaped\n\n    def wasserstein_distance_2d(self, x1, x2):\n        \"\"\"\n        Compute the Wasserstein distance between two 2D point clouds.\n\n        This method calculates the Earth Mover's Distance (EMD), also known as the\n        Wasserstein distance, between two sets of 2D points. It assumes that both\n        point clouds have equal weights assigned to each point. This distance is used\n        to evaluate how well the generated data distribution matches the real data\n        distribution, ensuring the synthetic data retains statistical similarity.\n\n        Args:\n            x1 (torch.Tensor): The first point cloud, represented as a batch of 2D points.\n            x2 (torch.Tensor): The second point cloud, represented as a batch of 2D points.\n\n        Returns:\n            float: The Wasserstein distance between the two point clouds.\n        \"\"\"\n        batch_size = x1.shape[0]\n\n        ab = torch.ones(batch_size) / batch_size\n        ab = ab.to(self.device)\n\n        M = ot.dist(x1, x2)\n\n        return ot.emd2(ab, ab, M)\n\n    def wasserstein_loss_mfs(self, mfs1, mfs2, average=True):\n        \"\"\"\n        Calculates the Wasserstein loss between two sets of meta-feature statistics (MFS).\n\n        This method quantifies the statistical similarity between the real and synthetic\n        data distributions by computing the Wasserstein distance between corresponding\n        feature pairs in the input MFS sets. This loss is used to train the generator\n        to produce synthetic data that closely matches the statistical properties of\n        the real data.\n\n        Args:\n            mfs1 (torch.Tensor): The first set of meta-feature statistics, representing\n                the real data distribution.\n            mfs2 (torch.Tensor): The second set of meta-feature statistics, representing\n                the synthetic data distribution.\n            average (bool, optional): A boolean indicating whether to return the average\n                loss (True) or a tensor of individual losses (False). Defaults to True.\n\n        Returns:\n            torch.Tensor or float: If `average` is True, returns the average\n                Wasserstein loss as a float. Otherwise, returns a tensor containing the\n                individual Wasserstein distances for each feature.\n        \"\"\"\n        # total = 0\n        n_features = mfs1.shape[0]\n\n        wsds = []\n        for first, second in zip(mfs1, mfs2):\n            wsd = self.wasserstein_distance_2d(first, second)\n            wsds.append(wsd)\n            # total += wsd\n\n        # print_debug = [[i, j.cpu().detach()] for i, j in zip(self.subset_mfs, wsds)]\n        # print(*print_debug,\n        #       sep='\\n')\n        if average:\n            return sum(wsds) / n_features\n        else:\n            return torch.stack(wsds).to(self.device)\n\n    def _generator_train_iteration(self, data):\n        \"\"\"\n        Generates synthetic data in one training step and optimizes the generator network.\n\n        This method produces data that is indistinguishable from real data based on the\n        discriminator's feedback and the matching of meta-feature statistics. The generator\n        is optimized to minimize both adversarial loss and MFS preservation loss.\n\n        Args:\n            data (torch.Tensor): A batch of real data used to determine batch size.\n\n        Returns:\n            None. The generator's parameters are updated to minimize the combined adversarial\n                and meta-feature statistics loss. The losses are also recorded.\n        \"\"\"\n        self.G_opt.zero_grad()\n\n        # Get generated data\n        batch_size = data.size(0)\n        generated_variates = []\n        for _ in range(self.sample_number):\n            generated_data = self.sample_generator(batch_size)\n\n            generated_data.requires_grad_(True)\n            generated_data.retain_grad()\n\n            # generated_data = generated_data.to(self.device)\n            generated_variates.append(generated_data)\n\n        # Calculate loss and optimize\n        d_generated = self.D(generated_variates[0])\n\n        fake_mfs = [self.calculate_mfs_torch(X) for X in generated_variates]\n        fake_mfs = self.reshape_mfs_from_variates(fake_mfs)\n\n        if isinstance(self.mfs_lambda, list):\n            mfs_lambda = torch.Tensor(self.mfs_lambda).to(self.device)\n            mfs_dist = self.wasserstein_loss_mfs(\n                fake_mfs, self.target_mfs[\"other_mfs\"], average=False\n            )\n\n            loss_mfs = mfs_lambda @ mfs_dist\n        elif isinstance(self.mfs_lambda, float):\n            mfs_dist = self.wasserstein_loss_mfs(\n                fake_mfs, self.target_mfs[\"other_mfs\"], average=True\n            )\n            loss_mfs = self.mfs_lambda * mfs_dist\n        else:\n            raise TypeError(\"mfs_lambda must be either a list or a float\")\n\n        g_loss = -d_generated.mean() + loss_mfs\n\n        if PLOT_GRAPH:\n            if not os.path.isfile(\"mod_computation_graph_G_loss.png\"):\n                make_dot(g_loss, show_attrs=True).render(\n                    \"mod_computation_graph_G_loss\", format=\"png\"\n                )\n\n        g_loss.backward()\n        self.G_opt.step()\n\n        # Record loss\n        self.G_loss = g_loss\n        self.mfs_loss = loss_mfs\n\n    @staticmethod\n    def plot_grad_flow(named_parameters, title=\"Gradient flow\"):\n        \"\"\"\n        Plots the gradient flow through the layers of a neural network to assess training dynamics.\n\n        This method calculates and visualizes the average gradient magnitude\n        for each layer of the network, excluding bias parameters. By observing the gradient flow,\n        one can identify layers that might be hindering the learning process due to vanishing\n        or exploding gradients, ensuring stable and effective training by maintaining data utility.\n\n        Args:\n            named_parameters: An iterator of tuples containing layer names and\n                parameter tensors.\n            title: The title of the plot. Defaults to \"Gradient flow\".\n\n        Returns:\n            matplotlib.figure.Figure: A matplotlib figure containing the gradient flow plot.\n        \"\"\"\n        ave_grads = []\n        layers = []\n        for n, p in named_parameters:\n            if p.requires_grad and p.grad is not None and \"bias\" not in n:\n                layers.append(n)\n                ave_grads.append(p.grad.abs().mean().item())\n\n        fig = plt.figure(figsize=(10, 5))\n        plt.plot(ave_grads, alpha=0.7, marker=\"o\", color=\"c\")\n        plt.hlines(0, 0, len(ave_grads), linewidth=1, color=\"k\")\n        plt.xticks(rotation=\"vertical\")\n        plt.xticks(range(len(layers)), layers, rotation=\"vertical\", fontsize=8)\n        plt.xlabel(\"Layer\")\n        plt.ylabel(\"Avg Gradient Magnitude\")\n        plt.title(title)\n        plt.grid(True)\n        plt.tight_layout()\n        plt.close(fig)\n        return fig\n\n    def plot_qq_plot(self, mfs_batch):\n        \"\"\"\n        Plots a quantile-quantile (QQ) plot to compare MFS distributions.\n\n        This method generates a QQ plot to visually assess how well the generated MFS from\n        a batch matches the distribution of the target MFS. It also plots a histogram of the\n        target MFS to visualize its distribution. The QQ plot helps determine if the GAN is\n        effectively learning to reproduce the statistical properties of the real data.\n\n        Args:\n            mfs_batch: A batch of generated MFS to compare against the target distribution.\n\n        Returns:\n            matplotlib.figure.Figure: The matplotlib figure containing the QQ plot.\n        \"\"\"\n        detached_target = (\n            self.target_mfs[\"other_mfs\"].cpu().detach().numpy().reshape(-1, 2)\n        )\n        mfs_batch_ = mfs_batch.reshape(-1, 2)\n        plt.figure()\n        plt.hist(detached_target)\n        fig = sm.qqplot_2samples(data1=detached_target, data2=mfs_batch_, line=\"45\")\n        plt.tight_layout()\n        plt.close(fig)\n        return fig\n\n    def train(self, data_loader, epochs, plot_freq):\n        \"\"\"\n        Trains the GAN model to generate synthetic data that mimics the statistical properties of the real data.\n\n        The training process involves updating the generator and discriminator networks iteratively\n        to improve the quality and utility of the generated samples. The method also tracks\n        various metrics and visualizations to monitor the training progress and evaluate the\n        performance of the GAN.\n\n        Args:\n            data_loader: The data loader providing batches of real data for training.\n            epochs: The number of training epochs to perform.\n            plot_freq: The frequency (in epochs) at which to generate and track plots for\n                monitoring training progress.\n\n        Returns:\n            None. The method trains the GAN model in place, updating the generator and\n                discriminator networks.\n        \"\"\"\n        pca = False\n        self.mfs_manager.change_device(self.device)\n        pbar = tqdm.tqdm(range(epochs), total=epochs, disable=self.disable)\n        self.loss_values = pd.DataFrame()\n        self.num_batches_per_epoch = len(data_loader)\n\n        for epoch in pbar:\n            self._train_epoch(data_loader)\n            pbar.set_description(f\"Epoch {epoch}\")\n\n            real_data_sample = next(iter(data_loader))\n            # samples = [self.sample_generator(self.batch_size) for _ in range(self.sample_number)]\n            samples = self.sample_generator(self.batch_size).cpu().detach().numpy()\n\n            self.aim_track.track(self.G_loss.item(), name=\"loss G\", epoch=epoch)\n            self.aim_track.track(self.D_loss.item(), name=\"loss D\", epoch=epoch)\n            self.aim_track.track(self.mfs_loss, name=\"loss MFS\", epoch=epoch)\n            self.aim_track.track(\n                self.total_grad_norm(self.G), name=\"total_norm_G\", epoch=epoch\n            )\n            self.aim_track.track(\n                self.total_grad_norm(self.D), name=\"total_norm_D\", epoch=epoch\n            )\n            self.aim_track.track(self.GP_grad_norm, name=\"GP_grad_norm\", epoch=epoch)\n\n            if epoch % plot_freq == 0:\n                fig = plt.figure()\n                if samples.shape[1] &gt; 2:\n                    pca = True\n                    pca = PCA(n_components=2)\n                    samples = pca.fit_transform(samples[:, :-1])\n                    real_data_sample = pca.fit_transform(real_data_sample[:, :-1])\n\n                plt.scatter(samples[:, 0], samples[:, 1], label=\"Synthetic\", alpha=0.3)\n                plt.scatter(\n                    real_data_sample[:, 0],\n                    real_data_sample[:, 1],\n                    label=\"Real data\",\n                    alpha=0.3,\n                )\n\n                if pca:\n                    plt.title(f\"Explained var: {sum(pca.explained_variance_ratio_)}\")\n\n                plt.legend()\n                plt.close(fig)\n\n                aim_fig = Image(fig)\n                if epoch % plot_freq == 0:\n                    self.aim_track.track(aim_fig, epoch=epoch, name=\"progress\")\n                # fig = plt.figure()\n                # plt.scatter(samples[0].cpu().detach().numpy()[:, 0], samples[0].cpu().detach().numpy()[:, 1],\n                #             label=\"Synthetic\", alpha=0.2)\n                # plt.scatter(real_data_sample[:, 0], real_data_sample[:, 1],\n                #             label=\"Real data\", alpha=0.2)\n                #\n                # plt.legend()\n                # plt.close(fig)\n\n                # aim_fig = Image(fig)\n                # self.aim_track.track(aim_fig, epoch=epoch, name=\"progress\")\n\n                fig_G = self.plot_grad_flow(\n                    self.G.named_parameters(), title=\"G gradient flow\"\n                )\n                fig_D = self.plot_grad_flow(\n                    self.D.named_parameters(), title=\"D gradient flow\"\n                )\n\n                # fig_mfs_distr = self.plot_qq_plot(\n                #     mfs_batch=np.asarray([self.calculate_mfs_torch(X).cpu().detach().numpy() for X in samples]))\n\n                aim_fig_G = Image(fig_G)\n                aim_fig_D = Image(fig_D)\n                # aim_fig_mfs_distr = Image(fig_mfs_distr)\n\n                # self.aim_track.track(aim_fig_mfs_distr, epoch=epoch, name=\"qq plot\")\n                self.aim_track.track(aim_fig_G, epoch=epoch, name=\"G grad flow\")\n                self.aim_track.track(aim_fig_D, epoch=epoch, name=\"D grad flow\")\n</code></pre>"},{"location":"wgan_gp/training/#wgan_gp.training.TrainerModified.__init__","title":"<code>__init__(mfs_lambda, subset_mfs, target_mfs, sample_number, **kwargs)</code>","text":"<p>Initializes the TrainerModified class with Meta-Feature Statistics preservation.</p> <p>This class configures the training process for the GAN, focusing on preserving meta-feature statistics to enhance the utility of generated synthetic data. It sets up the parameters that guide the MFS preservation process, ensuring the generated data retains key statistical characteristics of the real data.</p> <p>Parameters:</p> Name Type Description Default <code>mfs_lambda</code> <code>float or list</code> <p>Lambda value(s) for MFS loss weighting, controlling the strength of the meta-feature preservation regularization.</p> required <code>subset_mfs</code> <code>list</code> <p>Subset of meta-features to preserve, defining which statistical properties to focus on during training.</p> required <code>target_mfs</code> <code>dict</code> <p>Target MFS distributions, specifying the desired meta-feature distributions in the generated data. Defaults to {\"other_mfs\": 0} if not provided.</p> required <code>sample_number</code> <code>int</code> <p>Number of variates to use during MFS calculation, influencing the stability and accuracy of meta-feature estimation.</p> required <code>**kwargs</code> <p>Additional keyword arguments passed to the parent Trainer class.</p> <code>{}</code> <p>The method initializes the training process by setting up the meta-feature statistics (MFS) parameters. This setup is crucial for guiding the GAN to generate synthetic data that not only resembles the real data visually but also maintains its statistical utility. The target_mfs parameter allows specifying the desired distribution of meta-features in the generated data, ensuring that the synthetic data preserves important statistical properties for downstream tasks.</p> Source code in <code>wgan_gp/training.py</code> <pre><code>def __init__(self, mfs_lambda, subset_mfs, target_mfs, sample_number, **kwargs):\n    \"\"\"\n    Initializes the TrainerModified class with Meta-Feature Statistics preservation.\n\n    This class configures the training process for the GAN, focusing on preserving\n    meta-feature statistics to enhance the utility of generated synthetic data. It sets up\n    the parameters that guide the MFS preservation process, ensuring the generated data\n    retains key statistical characteristics of the real data.\n\n    Args:\n        mfs_lambda (float or list): Lambda value(s) for MFS loss weighting, controlling\n            the strength of the meta-feature preservation regularization.\n        subset_mfs (list): Subset of meta-features to preserve, defining which statistical\n            properties to focus on during training.\n        target_mfs (dict): Target MFS distributions, specifying the desired meta-feature\n            distributions in the generated data. Defaults to {\"other_mfs\": 0} if not provided.\n        sample_number (int): Number of variates to use during MFS calculation, influencing\n            the stability and accuracy of meta-feature estimation.\n        **kwargs: Additional keyword arguments passed to the parent Trainer class.\n\n    The method initializes the training process by setting up the meta-feature statistics (MFS)\n    parameters. This setup is crucial for guiding the GAN to generate synthetic data that not\n    only resembles the real data visually but also maintains its statistical utility. The\n    target_mfs parameter allows specifying the desired distribution of meta-features in the\n    generated data, ensuring that the synthetic data preserves important statistical properties\n    for downstream tasks.\n    \"\"\"\n    super(TrainerModified, self).__init__(**kwargs)\n    self.mfs_lambda = mfs_lambda\n    self.subset_mfs = subset_mfs\n\n    if not target_mfs:\n        target_mfs = {\"other_mfs\": 0}\n\n    self.target_mfs = target_mfs\n\n    if \"other_mfs\" in target_mfs.keys():\n        if isinstance(target_mfs[\"other_mfs\"], torch.Tensor):\n            self.target_mfs[\"other_mfs\"] = target_mfs[\"other_mfs\"].to(self.device)\n\n    self.mfs_manager = MFEToTorch()\n    self.wasserstein_dist_func = WassersteinDistance(q=2)\n    self.sample_number = sample_number\n</code></pre>"},{"location":"wgan_gp/training/#wgan_gp.training.TrainerModified.calculate_mfs_torch","title":"<code>calculate_mfs_torch(X, y=None)</code>","text":"<p>Calculates the meta-feature statistics (MFS) to quantify statistical properties for preserving data utility in GAN-generated synthetic data.</p> <p>This method leverages the MFS manager to assess various statistical properties of the input tensor X, optionally conditioned on a target tensor y. This helps in understanding which statistical characteristics are most important for preserving data utility in synthetic samples.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>The input tensor representing the synthetic data features.</p> required <code>y</code> <code>Tensor</code> <p>The target tensor representing the corresponding target variable. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The calculated MFS values, moved to the specified device. These values represent various statistical properties of the data (correlation, covariance, eigenvalues, etc.), which are used to guide the generator's learning process and ensure statistical fidelity.</p> Source code in <code>wgan_gp/training.py</code> <pre><code>def calculate_mfs_torch(\n    self, X: torch.Tensor, y: torch.Tensor = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Calculates the meta-feature statistics (MFS) to quantify statistical properties for\n    preserving data utility in GAN-generated synthetic data.\n\n    This method leverages the MFS manager to assess various statistical properties of the\n    input tensor X, optionally conditioned on a target tensor y. This helps in understanding\n    which statistical characteristics are most important for preserving data utility in\n    synthetic samples.\n\n    Args:\n        X (torch.Tensor): The input tensor representing the synthetic data features.\n        y (torch.Tensor, optional): The target tensor representing the corresponding\n            target variable. Defaults to None.\n\n    Returns:\n        torch.Tensor: The calculated MFS values, moved to the specified device.\n            These values represent various statistical properties of the data (correlation,\n            covariance, eigenvalues, etc.), which are used to guide the generator's\n            learning process and ensure statistical fidelity.\n    \"\"\"\n    return self.mfs_manager.get_mfs(X, y, subset=self.subset_mfs).to(self.device)\n</code></pre>"},{"location":"wgan_gp/training/#wgan_gp.training.TrainerModified.compute_loss_on_variates_wasserstein","title":"<code>compute_loss_on_variates_wasserstein(fake_distribution)</code>","text":"<p>Computes the Wasserstein loss to align generated and real data distributions.</p> <p>This method calculates the Wasserstein distance between the target meta-feature statistics (MFS) and the MFS generated from the fake data distribution. It first calculates the MFS for each variate in the fake distribution, reshapes them, and then computes the Wasserstein distance using the specified distance function. This loss encourages the generator to produce data with similar statistical properties to the real data, enhancing the utility of the synthetic data for downstream tasks.</p> <p>Parameters:</p> Name Type Description Default <code>fake_distribution</code> <p>A list of tensors representing the generated data distribution. Each tensor represents a variate.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The Wasserstein distance between the target MFS and the generated MFS.</p> Source code in <code>wgan_gp/training.py</code> <pre><code>def compute_loss_on_variates_wasserstein(self, fake_distribution):\n    \"\"\"\n    Computes the Wasserstein loss to align generated and real data distributions.\n\n    This method calculates the Wasserstein distance between the target meta-feature\n    statistics (MFS) and the MFS generated from the fake data distribution. It first\n    calculates the MFS for each variate in the fake distribution, reshapes them, and\n    then computes the Wasserstein distance using the specified distance function. This\n    loss encourages the generator to produce data with similar statistical properties\n    to the real data, enhancing the utility of the synthetic data for downstream tasks.\n\n    Args:\n        fake_distribution: A list of tensors representing the generated data distribution.\n            Each tensor represents a variate.\n\n    Returns:\n        torch.Tensor: The Wasserstein distance between the target MFS and the generated MFS.\n    \"\"\"\n    fake_mfs = [self.calculate_mfs_torch(X) for X in fake_distribution]\n    fake_mfs = self.reshape_mfs_from_variates(fake_mfs)\n\n    # mfs_to_track = fake_mfs.clone()\n    # self.mfs_to_track = mfs_to_track.mean(dim=1).cpu().detach().numpy().round(5).tolist()\n    return self.wasserstein_dist_func(self.target_mfs[\"other_mfs\"], fake_mfs)\n</code></pre>"},{"location":"wgan_gp/training/#wgan_gp.training.TrainerModified.plot_grad_flow","title":"<code>plot_grad_flow(named_parameters, title='Gradient flow')</code>  <code>staticmethod</code>","text":"<p>Plots the gradient flow through the layers of a neural network to assess training dynamics.</p> <p>This method calculates and visualizes the average gradient magnitude for each layer of the network, excluding bias parameters. By observing the gradient flow, one can identify layers that might be hindering the learning process due to vanishing or exploding gradients, ensuring stable and effective training by maintaining data utility.</p> <p>Parameters:</p> Name Type Description Default <code>named_parameters</code> <p>An iterator of tuples containing layer names and parameter tensors.</p> required <code>title</code> <p>The title of the plot. Defaults to \"Gradient flow\".</p> <code>'Gradient flow'</code> <p>Returns:</p> Type Description <p>matplotlib.figure.Figure: A matplotlib figure containing the gradient flow plot.</p> Source code in <code>wgan_gp/training.py</code> <pre><code>@staticmethod\ndef plot_grad_flow(named_parameters, title=\"Gradient flow\"):\n    \"\"\"\n    Plots the gradient flow through the layers of a neural network to assess training dynamics.\n\n    This method calculates and visualizes the average gradient magnitude\n    for each layer of the network, excluding bias parameters. By observing the gradient flow,\n    one can identify layers that might be hindering the learning process due to vanishing\n    or exploding gradients, ensuring stable and effective training by maintaining data utility.\n\n    Args:\n        named_parameters: An iterator of tuples containing layer names and\n            parameter tensors.\n        title: The title of the plot. Defaults to \"Gradient flow\".\n\n    Returns:\n        matplotlib.figure.Figure: A matplotlib figure containing the gradient flow plot.\n    \"\"\"\n    ave_grads = []\n    layers = []\n    for n, p in named_parameters:\n        if p.requires_grad and p.grad is not None and \"bias\" not in n:\n            layers.append(n)\n            ave_grads.append(p.grad.abs().mean().item())\n\n    fig = plt.figure(figsize=(10, 5))\n    plt.plot(ave_grads, alpha=0.7, marker=\"o\", color=\"c\")\n    plt.hlines(0, 0, len(ave_grads), linewidth=1, color=\"k\")\n    plt.xticks(rotation=\"vertical\")\n    plt.xticks(range(len(layers)), layers, rotation=\"vertical\", fontsize=8)\n    plt.xlabel(\"Layer\")\n    plt.ylabel(\"Avg Gradient Magnitude\")\n    plt.title(title)\n    plt.grid(True)\n    plt.tight_layout()\n    plt.close(fig)\n    return fig\n</code></pre>"},{"location":"wgan_gp/training/#wgan_gp.training.TrainerModified.plot_qq_plot","title":"<code>plot_qq_plot(mfs_batch)</code>","text":"<p>Plots a quantile-quantile (QQ) plot to compare MFS distributions.</p> <p>This method generates a QQ plot to visually assess how well the generated MFS from a batch matches the distribution of the target MFS. It also plots a histogram of the target MFS to visualize its distribution. The QQ plot helps determine if the GAN is effectively learning to reproduce the statistical properties of the real data.</p> <p>Parameters:</p> Name Type Description Default <code>mfs_batch</code> <p>A batch of generated MFS to compare against the target distribution.</p> required <p>Returns:</p> Type Description <p>matplotlib.figure.Figure: The matplotlib figure containing the QQ plot.</p> Source code in <code>wgan_gp/training.py</code> <pre><code>def plot_qq_plot(self, mfs_batch):\n    \"\"\"\n    Plots a quantile-quantile (QQ) plot to compare MFS distributions.\n\n    This method generates a QQ plot to visually assess how well the generated MFS from\n    a batch matches the distribution of the target MFS. It also plots a histogram of the\n    target MFS to visualize its distribution. The QQ plot helps determine if the GAN is\n    effectively learning to reproduce the statistical properties of the real data.\n\n    Args:\n        mfs_batch: A batch of generated MFS to compare against the target distribution.\n\n    Returns:\n        matplotlib.figure.Figure: The matplotlib figure containing the QQ plot.\n    \"\"\"\n    detached_target = (\n        self.target_mfs[\"other_mfs\"].cpu().detach().numpy().reshape(-1, 2)\n    )\n    mfs_batch_ = mfs_batch.reshape(-1, 2)\n    plt.figure()\n    plt.hist(detached_target)\n    fig = sm.qqplot_2samples(data1=detached_target, data2=mfs_batch_, line=\"45\")\n    plt.tight_layout()\n    plt.close(fig)\n    return fig\n</code></pre>"},{"location":"wgan_gp/training/#wgan_gp.training.TrainerModified.reshape_mfs_from_variates","title":"<code>reshape_mfs_from_variates(mfs_from_variates)</code>  <code>staticmethod</code>","text":"<p>Reshapes a list of meta-feature statistics from variates into a tensor for comparison.</p> <p>The input list <code>mfs_from_variates</code> contains MFS values, which are stacked and then transposed to create the reshaped tensor. This reshaping facilitates the calculation of metrics and topological analysis needed to evaluate the quality and utility of the generated synthetic data.</p> <p>Parameters:</p> Name Type Description Default <code>mfs_from_variates</code> <code>list</code> <p>A list of meta-feature statistics from variates.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: A reshaped tensor where the first dimension corresponds to the variates and the second dimension corresponds to the MFS values. This format is required for subsequent analysis and comparison of real and synthetic data characteristics.</p> Source code in <code>wgan_gp/training.py</code> <pre><code>@staticmethod\ndef reshape_mfs_from_variates(mfs_from_variates: list):\n    \"\"\"\n    Reshapes a list of meta-feature statistics from variates into a tensor for comparison.\n\n    The input list `mfs_from_variates` contains MFS values, which are stacked\n    and then transposed to create the reshaped tensor. This reshaping\n    facilitates the calculation of metrics and topological analysis\n    needed to evaluate the quality and utility of the generated synthetic data.\n\n    Args:\n        mfs_from_variates: A list of meta-feature statistics from variates.\n\n    Returns:\n        torch.Tensor: A reshaped tensor where the first dimension corresponds\n            to the variates and the second dimension corresponds to the MFS values.\n            This format is required for subsequent analysis and comparison\n            of real and synthetic data characteristics.\n    \"\"\"\n    stacked = torch.stack(mfs_from_variates)\n    reshaped = stacked.transpose(0, 1)\n    return reshaped\n</code></pre>"},{"location":"wgan_gp/training/#wgan_gp.training.TrainerModified.sample_from_tensor","title":"<code>sample_from_tensor(tensor, n_samples)</code>  <code>staticmethod</code>","text":"<p>Samples a subset of data points from a given tensor.</p> <p>This is useful for creating smaller, representative datasets for tasks such as evaluating model performance on a subset of the data or for visualization purposes.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor from which to sample.</p> required <code>n_samples</code> <code>int</code> <p>The number of data points to sample from the tensor.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: A new tensor containing the sampled data points. The sampled data maintains the original data's structure while reducing its size, which is important for efficient analysis and evaluation.</p> Source code in <code>wgan_gp/training.py</code> <pre><code>@staticmethod\ndef sample_from_tensor(tensor, n_samples):\n    \"\"\"\n    Samples a subset of data points from a given tensor.\n\n    This is useful for creating smaller, representative datasets for tasks such as\n    evaluating model performance on a subset of the data or for visualization purposes.\n\n    Args:\n        tensor (torch.Tensor): The input tensor from which to sample.\n        n_samples (int): The number of data points to sample from the tensor.\n\n    Returns:\n        torch.Tensor: A new tensor containing the sampled data points.\n            The sampled data maintains the original data's structure while reducing its size,\n            which is important for efficient analysis and evaluation.\n    \"\"\"\n    indices = torch.randperm(tensor.size(0))\n    indices_trunc = indices[:n_samples]\n    sampled_tensor = tensor[indices_trunc]\n    return sampled_tensor\n</code></pre>"},{"location":"wgan_gp/training/#wgan_gp.training.TrainerModified.total_grad_norm","title":"<code>total_grad_norm(model)</code>  <code>staticmethod</code>","text":"<p>Computes the total gradient norm of a model's parameters.</p> <p>Calculates the L2 norm of the gradients across all parameters in the model. This is useful for monitoring training and detecting potential issues like exploding gradients, ensuring stable training during synthetic data generation. By monitoring the gradient norm, we can ensure the generator and discriminator are learning effectively and preventing mode collapse, which is crucial for producing high-quality synthetic data.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The model whose gradients are to be analyzed.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The total gradient norm.</p> Source code in <code>wgan_gp/training.py</code> <pre><code>@staticmethod\ndef total_grad_norm(model):\n    \"\"\"\n    Computes the total gradient norm of a model's parameters.\n\n    Calculates the L2 norm of the gradients across all parameters in the model.\n    This is useful for monitoring training and detecting potential issues like\n    exploding gradients, ensuring stable training during synthetic data generation.\n    By monitoring the gradient norm, we can ensure the generator and discriminator\n    are learning effectively and preventing mode collapse, which is crucial for\n    producing high-quality synthetic data.\n\n    Args:\n        model: The model whose gradients are to be analyzed.\n\n    Returns:\n        float: The total gradient norm.\n    \"\"\"\n    total_norm = 0\n    for p in model.parameters():\n        if p.grad is not None:\n            param_norm = p.grad.data.norm(2)\n            total_norm += param_norm.item() ** 2\n    return total_norm**0.5\n</code></pre>"},{"location":"wgan_gp/training/#wgan_gp.training.TrainerModified.train","title":"<code>train(data_loader, epochs, plot_freq)</code>","text":"<p>Trains the GAN model to generate synthetic data that mimics the statistical properties of the real data.</p> <p>The training process involves updating the generator and discriminator networks iteratively to improve the quality and utility of the generated samples. The method also tracks various metrics and visualizations to monitor the training progress and evaluate the performance of the GAN.</p> <p>Parameters:</p> Name Type Description Default <code>data_loader</code> <p>The data loader providing batches of real data for training.</p> required <code>epochs</code> <p>The number of training epochs to perform.</p> required <code>plot_freq</code> <p>The frequency (in epochs) at which to generate and track plots for monitoring training progress.</p> required <p>Returns:</p> Type Description <p>None. The method trains the GAN model in place, updating the generator and discriminator networks.</p> Source code in <code>wgan_gp/training.py</code> <pre><code>def train(self, data_loader, epochs, plot_freq):\n    \"\"\"\n    Trains the GAN model to generate synthetic data that mimics the statistical properties of the real data.\n\n    The training process involves updating the generator and discriminator networks iteratively\n    to improve the quality and utility of the generated samples. The method also tracks\n    various metrics and visualizations to monitor the training progress and evaluate the\n    performance of the GAN.\n\n    Args:\n        data_loader: The data loader providing batches of real data for training.\n        epochs: The number of training epochs to perform.\n        plot_freq: The frequency (in epochs) at which to generate and track plots for\n            monitoring training progress.\n\n    Returns:\n        None. The method trains the GAN model in place, updating the generator and\n            discriminator networks.\n    \"\"\"\n    pca = False\n    self.mfs_manager.change_device(self.device)\n    pbar = tqdm.tqdm(range(epochs), total=epochs, disable=self.disable)\n    self.loss_values = pd.DataFrame()\n    self.num_batches_per_epoch = len(data_loader)\n\n    for epoch in pbar:\n        self._train_epoch(data_loader)\n        pbar.set_description(f\"Epoch {epoch}\")\n\n        real_data_sample = next(iter(data_loader))\n        # samples = [self.sample_generator(self.batch_size) for _ in range(self.sample_number)]\n        samples = self.sample_generator(self.batch_size).cpu().detach().numpy()\n\n        self.aim_track.track(self.G_loss.item(), name=\"loss G\", epoch=epoch)\n        self.aim_track.track(self.D_loss.item(), name=\"loss D\", epoch=epoch)\n        self.aim_track.track(self.mfs_loss, name=\"loss MFS\", epoch=epoch)\n        self.aim_track.track(\n            self.total_grad_norm(self.G), name=\"total_norm_G\", epoch=epoch\n        )\n        self.aim_track.track(\n            self.total_grad_norm(self.D), name=\"total_norm_D\", epoch=epoch\n        )\n        self.aim_track.track(self.GP_grad_norm, name=\"GP_grad_norm\", epoch=epoch)\n\n        if epoch % plot_freq == 0:\n            fig = plt.figure()\n            if samples.shape[1] &gt; 2:\n                pca = True\n                pca = PCA(n_components=2)\n                samples = pca.fit_transform(samples[:, :-1])\n                real_data_sample = pca.fit_transform(real_data_sample[:, :-1])\n\n            plt.scatter(samples[:, 0], samples[:, 1], label=\"Synthetic\", alpha=0.3)\n            plt.scatter(\n                real_data_sample[:, 0],\n                real_data_sample[:, 1],\n                label=\"Real data\",\n                alpha=0.3,\n            )\n\n            if pca:\n                plt.title(f\"Explained var: {sum(pca.explained_variance_ratio_)}\")\n\n            plt.legend()\n            plt.close(fig)\n\n            aim_fig = Image(fig)\n            if epoch % plot_freq == 0:\n                self.aim_track.track(aim_fig, epoch=epoch, name=\"progress\")\n            # fig = plt.figure()\n            # plt.scatter(samples[0].cpu().detach().numpy()[:, 0], samples[0].cpu().detach().numpy()[:, 1],\n            #             label=\"Synthetic\", alpha=0.2)\n            # plt.scatter(real_data_sample[:, 0], real_data_sample[:, 1],\n            #             label=\"Real data\", alpha=0.2)\n            #\n            # plt.legend()\n            # plt.close(fig)\n\n            # aim_fig = Image(fig)\n            # self.aim_track.track(aim_fig, epoch=epoch, name=\"progress\")\n\n            fig_G = self.plot_grad_flow(\n                self.G.named_parameters(), title=\"G gradient flow\"\n            )\n            fig_D = self.plot_grad_flow(\n                self.D.named_parameters(), title=\"D gradient flow\"\n            )\n\n            # fig_mfs_distr = self.plot_qq_plot(\n            #     mfs_batch=np.asarray([self.calculate_mfs_torch(X).cpu().detach().numpy() for X in samples]))\n\n            aim_fig_G = Image(fig_G)\n            aim_fig_D = Image(fig_D)\n            # aim_fig_mfs_distr = Image(fig_mfs_distr)\n\n            # self.aim_track.track(aim_fig_mfs_distr, epoch=epoch, name=\"qq plot\")\n            self.aim_track.track(aim_fig_G, epoch=epoch, name=\"G grad flow\")\n            self.aim_track.track(aim_fig_D, epoch=epoch, name=\"D grad flow\")\n</code></pre>"},{"location":"wgan_gp/training/#wgan_gp.training.TrainerModified.wasserstein_distance_2d","title":"<code>wasserstein_distance_2d(x1, x2)</code>","text":"<p>Compute the Wasserstein distance between two 2D point clouds.</p> <p>This method calculates the Earth Mover's Distance (EMD), also known as the Wasserstein distance, between two sets of 2D points. It assumes that both point clouds have equal weights assigned to each point. This distance is used to evaluate how well the generated data distribution matches the real data distribution, ensuring the synthetic data retains statistical similarity.</p> <p>Parameters:</p> Name Type Description Default <code>x1</code> <code>Tensor</code> <p>The first point cloud, represented as a batch of 2D points.</p> required <code>x2</code> <code>Tensor</code> <p>The second point cloud, represented as a batch of 2D points.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The Wasserstein distance between the two point clouds.</p> Source code in <code>wgan_gp/training.py</code> <pre><code>def wasserstein_distance_2d(self, x1, x2):\n    \"\"\"\n    Compute the Wasserstein distance between two 2D point clouds.\n\n    This method calculates the Earth Mover's Distance (EMD), also known as the\n    Wasserstein distance, between two sets of 2D points. It assumes that both\n    point clouds have equal weights assigned to each point. This distance is used\n    to evaluate how well the generated data distribution matches the real data\n    distribution, ensuring the synthetic data retains statistical similarity.\n\n    Args:\n        x1 (torch.Tensor): The first point cloud, represented as a batch of 2D points.\n        x2 (torch.Tensor): The second point cloud, represented as a batch of 2D points.\n\n    Returns:\n        float: The Wasserstein distance between the two point clouds.\n    \"\"\"\n    batch_size = x1.shape[0]\n\n    ab = torch.ones(batch_size) / batch_size\n    ab = ab.to(self.device)\n\n    M = ot.dist(x1, x2)\n\n    return ot.emd2(ab, ab, M)\n</code></pre>"},{"location":"wgan_gp/training/#wgan_gp.training.TrainerModified.wasserstein_loss_mfs","title":"<code>wasserstein_loss_mfs(mfs1, mfs2, average=True)</code>","text":"<p>Calculates the Wasserstein loss between two sets of meta-feature statistics (MFS).</p> <p>This method quantifies the statistical similarity between the real and synthetic data distributions by computing the Wasserstein distance between corresponding feature pairs in the input MFS sets. This loss is used to train the generator to produce synthetic data that closely matches the statistical properties of the real data.</p> <p>Parameters:</p> Name Type Description Default <code>mfs1</code> <code>Tensor</code> <p>The first set of meta-feature statistics, representing the real data distribution.</p> required <code>mfs2</code> <code>Tensor</code> <p>The second set of meta-feature statistics, representing the synthetic data distribution.</p> required <code>average</code> <code>bool</code> <p>A boolean indicating whether to return the average loss (True) or a tensor of individual losses (False). Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <p>torch.Tensor or float: If <code>average</code> is True, returns the average Wasserstein loss as a float. Otherwise, returns a tensor containing the individual Wasserstein distances for each feature.</p> Source code in <code>wgan_gp/training.py</code> <pre><code>def wasserstein_loss_mfs(self, mfs1, mfs2, average=True):\n    \"\"\"\n    Calculates the Wasserstein loss between two sets of meta-feature statistics (MFS).\n\n    This method quantifies the statistical similarity between the real and synthetic\n    data distributions by computing the Wasserstein distance between corresponding\n    feature pairs in the input MFS sets. This loss is used to train the generator\n    to produce synthetic data that closely matches the statistical properties of\n    the real data.\n\n    Args:\n        mfs1 (torch.Tensor): The first set of meta-feature statistics, representing\n            the real data distribution.\n        mfs2 (torch.Tensor): The second set of meta-feature statistics, representing\n            the synthetic data distribution.\n        average (bool, optional): A boolean indicating whether to return the average\n            loss (True) or a tensor of individual losses (False). Defaults to True.\n\n    Returns:\n        torch.Tensor or float: If `average` is True, returns the average\n            Wasserstein loss as a float. Otherwise, returns a tensor containing the\n            individual Wasserstein distances for each feature.\n    \"\"\"\n    # total = 0\n    n_features = mfs1.shape[0]\n\n    wsds = []\n    for first, second in zip(mfs1, mfs2):\n        wsd = self.wasserstein_distance_2d(first, second)\n        wsds.append(wsd)\n        # total += wsd\n\n    # print_debug = [[i, j.cpu().detach()] for i, j in zip(self.subset_mfs, wsds)]\n    # print(*print_debug,\n    #       sep='\\n')\n    if average:\n        return sum(wsds) / n_features\n    else:\n        return torch.stack(wsds).to(self.device)\n</code></pre>"}]}